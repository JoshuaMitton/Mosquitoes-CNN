{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook provides the functionality to build, train, and test a CNN for predicting mosquito age, grouped age, species, and status.\n",
    "\n",
    "## Structure:\n",
    "* Import packages to be used.\n",
    "* Load mosquito data.\n",
    "* Define fucntions for plotting, visualisation, and logging.\n",
    "* Define a function to build the CNN.\n",
    "* Define a function to train the CNN.\n",
    "* Main section to organise data, define the CNN, and call the building and training of the CNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/anaconda3/envs/mosquitoes/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/josh/anaconda3/envs/mosquitoes/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/josh/anaconda3/envs/mosquitoes/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/josh/anaconda3/envs/mosquitoes/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/josh/anaconda3/envs/mosquitoes/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/josh/anaconda3/envs/mosquitoes/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pylab as pl\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "import pickle\n",
    "import random as rn\n",
    "import os\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras import layers, metrics\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.models import model_from_json, load_model\n",
    "from keras.regularizers import *\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# rand_seed = np.random.randint(low=0, high=100)\n",
    "rand_seed = 16\n",
    "print(rand_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "## The below is necessary for starting Numpy generated random numbers in a well-defined initial state.\n",
    "np.random.seed(42)\n",
    "\n",
    "## The below is necessary for starting core Python generated random numbers in a well-defined state.\n",
    "rn.seed(12345)\n",
    "\n",
    "## Force TensorFlow to use single thread.\n",
    "## Multiple threads are a potential source of\n",
    "## non-reproducible results.\n",
    "## For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "# session_conf = tf.ConfigProto(device_count = {'GPU':0}, intra_op_parallelism_threads=4) #session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "# session_conf = tf.ConfigProto(device_count = {'GPU':0}) #session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "#session_conf.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "## The below tf.set_random_seed() will make random number generation\n",
    "## in the TensorFlow backend have a well-defined initial state.\n",
    "## For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.35)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "The data file is created using Loco Mosquito:\n",
    "https://github.com/magonji/MIMI-project/blob/master/Loco%20mosquito%204.0.ipynb\n",
    "\n",
    "### The data file has headings: Species - Status - RearCnd - Age - Country- Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function used to create a new folder for the CNN outputs.\n",
    "Useful to stop forgetting to name a new folder when trying out a new model varient and overwriting a days training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_folder(fold, to_build = False):\n",
    "    if not os.path.isdir(fold):\n",
    "        if to_build == True:\n",
    "            os.mkdir(fold)\n",
    "        else:\n",
    "            print('Directory does not exists, not creating directory!')\n",
    "    else:\n",
    "        if to_build == True:\n",
    "            raise NameError('Directory already exists, cannot be created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for plotting confusion matrcies\n",
    "This normalizes the confusion matrix and ensures neat plotting for all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, output, save_path, model_name, fold,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          printout=False):\n",
    "\n",
    "    font = {'weight' : 'normal',\n",
    "            'size'   : 15}\n",
    "\n",
    "    matplotlib.rc('font', **font)\n",
    "\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        if printout:\n",
    "            print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        if printout:\n",
    "            print('Confusion matrix, without normalization')\n",
    "\n",
    "    if printout:\n",
    "        print(cm)\n",
    "    \n",
    "    plt.figure(figsize=(2.8,2.8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, vmin=0, vmax=1) # np.max(np.sum(cm, axis=1)))\n",
    "#     plt.title([title+' - '+model_name])\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.ylim(2.5,-0.5)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout(pad=0.3)\n",
    "#     plt.tight_layout()\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')\n",
    "    plt.savefig((save_path+\"Confusion_Matrix_\"+model_name+\"_\"+fold+\"_\"+output[1:]+\".pdf\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function used for visualizing outputs\n",
    "This splits the output data into the four categories before plotting the confusion matricies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for visualizing losses and metrics once the neural network fold is trained\n",
    "def visualize(histories, save_path, model_name, fold, classes, outputs, predicted, true):\n",
    "    # Sort out predictions and true labels\n",
    "    for label_predictions_arr, label_true_arr, classes, outputs in zip(predicted, true, classes, outputs):\n",
    "        classes_pred = np.argmax(label_predictions_arr, axis=-1)\n",
    "        classes_true = np.argmax(label_true_arr, axis=-1)\n",
    "        cnf_matrix = confusion_matrix(classes_true, classes_pred)\n",
    "        plot_confusion_matrix(cnf_matrix, classes, outputs, save_path, model_name, fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for logging data associated with the model\n",
    "def log_data(log, name, fold, save_path):\n",
    "    f = open((save_path+name+'_'+str(fold)+'_log.txt'), 'w')\n",
    "    np.savetxt(f, log)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fucntion for graphing the training data\n",
    "This fucntion creates tidy graphs of loss and accuracy as the models are training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_history(history, model_name, model_ver_num, fold, save_path):\n",
    "\n",
    "    font = {'weight' : 'normal',\n",
    "            'size'   : 18}\n",
    "\n",
    "    matplotlib.rc('font', **font)\n",
    "    \n",
    "    #not_validation = list(filter(lambda x: x[0:3] != \"val\", history.history.keys()))\n",
    "#     print('history.history.keys : {}'.format(history.history.keys()))\n",
    "    filtered = filter(lambda x: x[0:3] != \"val\", history.history.keys())\n",
    "    not_validation = list(filtered)\n",
    "    for i in not_validation:\n",
    "        plt.figure(figsize=(15,7))\n",
    "#         plt.title(i+\"/ \"+\"val_\"+i)\n",
    "        plt.plot(history.history[i], label=i)\n",
    "        plt.plot(history.history[\"val_\"+i], label=\"val_\"+i)\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(i)\n",
    "        plt.savefig(save_path +model_name+\"_\"+str(model_ver_num)+\"_\"+str(fold)+\"_\"+i)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funciton to create the CNN\n",
    "This function takes as an input a list of dictionaries. Each element in the list is a new hidden layer in the model. For each layer the dictionary defines the layer to be used.\n",
    "\n",
    "### Available options are:\n",
    "Convolutional Layer:\n",
    "* type = 'c'\n",
    "* filter = optional number of filters\n",
    "* kernel = optional size of the filters\n",
    "* stride = optional size of stride to take between filters\n",
    "* pooling = optional width of the max pooling\n",
    "* {'type':'c', 'filter':16, 'kernel':5, 'stride':1, 'pooling':2}\n",
    "\n",
    "dense layer:\n",
    "* type = 'd'\n",
    "* width = option width of the layer\n",
    "* {'type':'d', 'width':500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(model_shape, input_layer_dim):\n",
    "\n",
    "    regConst = 0.02\n",
    "    sgd = keras.optimizers.SGD(lr=0.003, decay=1e-5, momentum=0.9, nesterov=True, clipnorm=1.)\n",
    "    cce = 'categorical_crossentropy'\n",
    "\n",
    "    input_vec = Input(name='input', shape=(input_layer_dim,1))\n",
    "\n",
    "    for i, layerwidth in zip(range(len(model_shape)),model_shape):\n",
    "        if i == 0:\n",
    "            if model_shape[i]['type'] == 'c':\n",
    "                xd = Conv1D(name=('Conv'+str(i+1)), filters=model_shape[i]['filter'], \n",
    "                 kernel_size = model_shape[i]['kernel'], strides = model_shape[i]['stride'],\n",
    "                 activation = 'relu',\n",
    "                 kernel_regularizer=l2(regConst), \n",
    "                 kernel_initializer='he_normal')(input_vec)\n",
    "                xd = BatchNormalization(name=('batchnorm_'+str(i+1)))(xd)\n",
    "                xd = MaxPooling1D(pool_size=(model_shape[i]['pooling']))(xd)\n",
    "                \n",
    "            elif model_shape[i]['type'] == 'd':\n",
    "                xd = Dense(name=('d'+str(i+1)), units=model_shape[i]['width'], activation='relu', \n",
    "                 kernel_regularizer=l2(regConst), \n",
    "                 kernel_initializer='he_normal')(input_vec)\n",
    "                xd = BatchNormalization(name=('batchnorm_'+str(i+1)))(xd) \n",
    "                xd = Dropout(name=('dout'+str(i+1)), rate=0.5)(xd) \n",
    "                \n",
    "        else:\n",
    "            if model_shape[i]['type'] == 'c':\n",
    "                xd = Conv1D(name=('Conv'+str(i+1)), filters=model_shape[i]['filter'], \n",
    "                 kernel_size = model_shape[i]['kernel'], strides = model_shape[i]['stride'],\n",
    "                 activation = 'relu',\n",
    "                 kernel_regularizer=l2(regConst), \n",
    "                 kernel_initializer='he_normal')(xd)\n",
    "                xd = BatchNormalization(name=('batchnorm_'+str(i+1)))(xd)\n",
    "                xd = MaxPooling1D(pool_size=(model_shape[i]['pooling']))(xd)\n",
    "                \n",
    "            elif model_shape[i]['type'] == 'd':\n",
    "                if model_shape[i-1]['type'] == 'c':\n",
    "                    xd = Flatten()(xd)\n",
    "                    \n",
    "                xd = Dropout(name=('dout'+str(i+1)), rate=0.5)(xd)\n",
    "                xd = Dense(name=('d'+str(i+1)), units=model_shape[i]['width'], activation='relu', \n",
    "                 kernel_regularizer=l2(regConst), \n",
    "                 kernel_initializer='he_normal')(xd)\n",
    "                xd = BatchNormalization(name=('batchnorm_'+str(i+1)))(xd) \n",
    "        \n",
    "    \n",
    "#     xAge     = Dense(name = 'age', units = 17, \n",
    "#                      activation = 'softmax', \n",
    "#                      kernel_regularizer = l2(regConst), \n",
    "#                      kernel_initializer = 'he_normal')(xd)\n",
    "    xAgeGroup     = Dense(name = 'age_group', units = 3, \n",
    "                     activation = 'softmax', \n",
    "                     kernel_regularizer = l2(regConst), \n",
    "                     kernel_initializer = 'he_normal')(xd)\n",
    "    xSpecies = Dense(name ='species', units = 3, \n",
    "                     activation = 'softmax', \n",
    "                     kernel_regularizer = l2(regConst), \n",
    "                     kernel_initializer = 'he_normal')(xd)\n",
    "\n",
    "    outputs = []\n",
    "#     for i in ['xAge', 'xAgeGroup', 'xSpecies']:\n",
    "    for i in ['xAgeGroup', 'xSpecies']:\n",
    "        outputs.append(locals()[i])\n",
    "    model = Model(inputs = input_vec, outputs = outputs)\n",
    "    \n",
    "    model.compile(loss=cce, metrics=['acc'], \n",
    "                  optimizer=sgd)\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to train the model\n",
    "\n",
    "This function will split the data into training and validation and call the create models function. This fucntion returns the model and training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(model_to_test, save_path, SelectFreqs=False):\n",
    "\n",
    "    model_shape = model_to_test[\"model_shape\"][0]\n",
    "    model_name = model_to_test[\"model_name\"][0]\n",
    "#     input_layer_dim = model_to_test[\"input_layer_dim\"][0]\n",
    "    model_ver_num = model_to_test[\"model_ver_num\"][0]\n",
    "    fold = model_to_test[\"fold\"][0]\n",
    "    label = model_to_test[\"labels\"][0]\n",
    "    features = model_to_test[\"features\"][0]\n",
    "    classes = model_to_test[\"classes\"][0]\n",
    "    outputs = model_to_test[\"outputs\"][0]\n",
    "    compile_loss = model_to_test[\"compile_loss\"][0]\n",
    "    compile_metrics = model_to_test[\"compile_metrics\"][0]\n",
    "\n",
    "#     ## Split into training / testing\n",
    "#     test_splits = train_test_split(features, *(label), test_size=0.1, shuffle=True, random_state=rand_seed)\n",
    "#     ## Pack up data\n",
    "#     X_train = test_splits.pop(0)\n",
    "#     X_val = test_splits.pop(0)\n",
    "#     y_train = test_splits[::2]\n",
    "#     y_val = test_splits[1::2]\n",
    "    \n",
    "#     out_model = create_models(model_shape, input_layer_dim, SelectFreqs)\n",
    "#     out_model.summary()\n",
    "#     out_history = out_model.fit(x = X_train, \n",
    "#                             y = y_train,\n",
    "#                             batch_size = 128*16, \n",
    "#                             verbose = 0, \n",
    "#                             epochs = 8000,\n",
    "#                             validation_data = (X_val, y_val),\n",
    "#                             callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "#                                         patience=400, verbose=0, mode='auto'), \n",
    "#                                         CSVLogger(save_path+model_name+\"_\"+str(model_ver_num)+'.csv', append=True, separator=';')])\n",
    "#     scores = out_model.evaluate(X_val, y_val)\n",
    "#     print(out_model.metrics_names)\n",
    "\n",
    "    ## Kfold training\n",
    "    seed = rand_seed\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "    ## Split data into test and train\n",
    "    \n",
    "    model_ver_num = 0\n",
    "    cv_scores = []\n",
    "    best_score = 0\n",
    "    for train_index, val_index in kfold.split(features):\n",
    "        print('Fold {} Running'.format(model_ver_num))\n",
    "        \n",
    "        X_train, X_val = features[train_index], features[val_index]\n",
    "        y_train, y_val = list(map(lambda y:y[train_index], label)), list(map(lambda y:y[val_index], label))\n",
    "\n",
    "        model = create_models(model_shape, input_layer_dim)\n",
    "        if model_ver_num == 0:\n",
    "            model.summary()\n",
    "        \n",
    "        modsavedir = save_path+\"Model_\"+str(model_ver_num)+\"/\"\n",
    "        build_folder(modsavedir, True)\n",
    "\n",
    "        history = model.fit(x = X_train, \n",
    "                            y = y_train,\n",
    "                            batch_size = 128*16, \n",
    "                            verbose = 0, \n",
    "                            epochs = 8000,\n",
    "                            validation_data = (X_val, y_val),\n",
    "                            callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                        patience=400, verbose=0, mode='auto'), \n",
    "                                        CSVLogger(modsavedir+model_name+\"_\"+str(model_ver_num)+'.csv', append=True, separator=';')])\n",
    "        scores = model.evaluate(X_val, y_val)\n",
    "        \n",
    "        model.save((modsavedir+model_name+\"_\"+str(model_ver_num)+\"_\"+'Model.h5'))\n",
    "        graph_history(history, model_name, model_ver_num, 0, modsavedir)\n",
    "        \n",
    "#         print(model.metrics_names)\n",
    "#         print(scores)\n",
    "        if (scores[3] + scores[4]) > best_score:\n",
    "            out_model = model\n",
    "            out_history = history\n",
    "        \n",
    "        model_ver_num += 1\n",
    "        \n",
    "#         # Clear the Keras session, otherwise it will keep adding new\n",
    "#         # models to the same TensorFlow graph each time we create\n",
    "#         # a model with a different set of hyper-parameters.\n",
    "#         K.clear_session()\n",
    "\n",
    "#         # Delete the Keras model with these hyper-parameters from memory.\n",
    "#         del model\n",
    "        \n",
    "    out_model.save((save_path+model_name+\"_\"+'Model.h5'))\n",
    "    graph_history(out_history, model_name, 0, 0, save_path)\n",
    "    \n",
    "    return out_model, out_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main section\n",
    "\n",
    "Functionality:\n",
    "* Oganises the data into a format of lists of data, classes, labels.\n",
    "* Define the CNN to be built.\n",
    "* Define the KFold validation to be used.\n",
    "* Build a folder to output data into.\n",
    "* Standardize and oragnise data into training/testing.\n",
    "* Call the model training.\n",
    "* Organize outputs and call visualization for plotting and graphing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = \"Results_Paper/\"\n",
    "build_folder(outdir, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X : (4800, 1625)\n",
      "shape of y age : (4800,)\n",
      "shape of y age groups : (4800,)\n",
      "shape of y species : (4800,)\n",
      "shape of y status : (4800,)\n",
      "shape of X f : (2400, 1625)\n",
      "shape of y age f : (2400,)\n",
      "shape of y age groups f : (2400,)\n",
      "shape of y species f : (2400,)\n",
      "shape of y status f : (2400,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../../Data/mosquitoes_country_LM_5_0.dat\", '\\t')\n",
    "df.head(10)\n",
    "RearCnd_counts = df.groupby('RearCnd').size()\n",
    "\n",
    "df['AgeGroup'] = 0\n",
    "df['AgeGroup'] = np.where(df['Age']>10, 2, np.where(df['Age']>4, 1, 0))\n",
    "\n",
    "df_vf = df[df['RearCnd']=='VF']\n",
    "df_vf = df_vf[df_vf['Status']=='UN']\n",
    "df = df[df['RearCnd']!='VF']\n",
    "df = df[df['Status']!='UN']\n",
    "df_l = df[df['RearCnd']=='TL']\n",
    "df_l_g = df_l[df_l['Country']=='S']\n",
    "df_l_g_a = df_l_g[df_l_g['Species']=='AA']\n",
    "age_counts = df_l_g_a.groupby('AgeGroup').size()\n",
    "df_l_g_g = df_l_g[df_l_g['Species']=='AG']\n",
    "age_counts = df_l_g_g.groupby('AgeGroup').size()\n",
    "df_l_g_c = df_l_g[df_l_g['Species']=='AC']\n",
    "age_counts = df_l_g_c.groupby('AgeGroup').size()\n",
    "df_l_t = df_l[df_l['Country']=='T']\n",
    "df_l_t_a = df_l_t[df_l_t['Species']=='AA']\n",
    "age_counts = df_l_t_a.groupby('AgeGroup').size()\n",
    "df_l_t_g = df_l_t[df_l_t['Species']=='AG']\n",
    "age_counts = df_l_t_g.groupby('AgeGroup').size()\n",
    "df_l_b = df_l[df_l['Country']=='B']\n",
    "df_l_b_g = df_l_b[df_l_b['Species']=='AG']\n",
    "age_counts = df_l_b_g.groupby('AgeGroup').size()\n",
    "df_l_b_c = df_l_b[df_l_b['Species']=='AC']\n",
    "age_counts = df_l_b_c.groupby('AgeGroup').size()\n",
    "df_f = df[df['RearCnd']=='TF']\n",
    "df_f_t = df_f[df_f['Country']=='T']\n",
    "df_f_t_a = df_f_t[df_f_t['Species']=='AA']\n",
    "age_counts = df_f_t_a.groupby('AgeGroup').size()\n",
    "# df_f_t_g = df_f_t[df_f_t['Species']=='AG'] #There isn't any\n",
    "df_f_b = df_f[df_f['Country']=='B']\n",
    "df_f_b_g = df_f_b[df_f_b['Species']=='AG']\n",
    "age_counts = df_f_b_g.groupby('AgeGroup').size()\n",
    "df_f_b_c = df_f_b[df_f_b['Species']=='AC']\n",
    "age_counts = df_f_b_c.groupby('AgeGroup').size()\n",
    "df_vf_t = df_vf[df_vf['Country']=='T']\n",
    "df_vf_t_a = df_vf_t[df_vf_t['Species']=='AA']\n",
    "age_counts = df_vf_t_a.groupby('AgeGroup').size()\n",
    "df_vf_t_g = df_vf_t[df_vf_t['Species']=='AG']\n",
    "age_counts = df_vf_t_g.groupby('AgeGroup').size()\n",
    "df_vf_b = df_vf[df_vf['Country']=='B']\n",
    "df_vf_b_g = df_vf_b[df_vf_b['Species']=='AG']\n",
    "age_counts = df_vf_b_g.groupby('AgeGroup').size()\n",
    "df_vf_b_c = df_vf_b[df_vf_b['Species']=='AC']\n",
    "age_counts = df_vf_b_c.groupby('AgeGroup').size()\n",
    "\n",
    "size_inc = 400\n",
    "for age in range(3):\n",
    "    df_temp = df_l_t_a[df_l_t_a['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    if age == 0:\n",
    "        df_train = df_temp.iloc[index_df_temp_inc]\n",
    "#         df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "    else:\n",
    "        df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#         df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_t_g[df_l_t_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 400\n",
    "for age in range(3):\n",
    "    df_temp = df_l_b_g[df_l_b_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_b_c[df_l_b_c['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 300 # 50\n",
    "for age in range(3):\n",
    "    df_temp = df_f_t_a[df_f_t_a['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    if age == 0:\n",
    "        df_trainf = df_temp.iloc[index_df_temp_inc]\n",
    "#         df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "    else:\n",
    "        df_trainf = pd.concat([df_trainf, df_temp.iloc[index_df_temp_inc]])\n",
    "#     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_f_b_g[df_f_b_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_trainf = pd.concat([df_trainf, df_temp.iloc[index_df_temp_inc]])\n",
    "#     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_f_b_c[df_f_b_c['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_trainf = pd.concat([df_trainf, df_temp.iloc[index_df_temp_inc]])\n",
    "#     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 0\n",
    "for age in range(3):\n",
    "    df_temp = df_vf_t_a[df_vf_t_a['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    if age == 0:\n",
    "        df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "    else:\n",
    "        df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_vf_t_g[df_vf_t_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 0\n",
    "for age in range(3):\n",
    "    df_temp = df_vf_b_g[df_vf_b_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_vf_b_c[df_vf_b_c['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "\n",
    "X = df_train.iloc[:,6:-1]\n",
    "y_age = df_train[\"Age\"]\n",
    "y_age_groups = df_train[\"AgeGroup\"]\n",
    "y_species = df_train[\"Species\"]\n",
    "y_status = df_train[\"Status\"]\n",
    "X = np.asarray(X)\n",
    "y_age = np.asarray(y_age)\n",
    "y_age_groups = np.asarray(y_age_groups)\n",
    "y_species = np.asarray(y_species)\n",
    "y_status = np.asarray(y_status)\n",
    "\n",
    "print('shape of X : {}'.format(X.shape))\n",
    "print('shape of y age : {}'.format(y_age.shape))\n",
    "print('shape of y age groups : {}'.format(y_age_groups.shape))\n",
    "print('shape of y species : {}'.format(y_species.shape))\n",
    "print('shape of y status : {}'.format(y_status.shape))\n",
    "\n",
    "Xf = df_trainf.iloc[:,6:-1]\n",
    "y_agef = df_trainf[\"Age\"]\n",
    "y_age_groupsf = df_trainf[\"AgeGroup\"]\n",
    "y_speciesf = df_trainf[\"Species\"]\n",
    "y_statusf = df_trainf[\"Status\"]\n",
    "Xf = np.asarray(Xf)\n",
    "y_agef = np.asarray(y_agef)\n",
    "y_age_groupsf = np.asarray(y_age_groupsf)\n",
    "y_speciesf = np.asarray(y_speciesf)\n",
    "y_statusf = np.asarray(y_statusf)\n",
    "\n",
    "print('shape of X f : {}'.format(Xf.shape))\n",
    "print('shape of y age f : {}'.format(y_agef.shape))\n",
    "print('shape of y age groups f : {}'.format(y_age_groupsf.shape))\n",
    "print('shape of y species f : {}'.format(y_speciesf.shape))\n",
    "print('shape of y status f : {}'.format(y_statusf.shape))\n",
    "\n",
    "X_vf = df_test.iloc[:,6:-1]\n",
    "y_age_vf = df_test[\"Age\"]\n",
    "y_age_groups_vf = df_test[\"AgeGroup\"]\n",
    "y_species_vf = df_test[\"Species\"]\n",
    "y_status_vf = df_test[\"Status\"]\n",
    "X_vf = np.asarray(X_vf)\n",
    "y_age_vf = np.asarray(y_age_vf)\n",
    "y_age_groups_vf = np.asarray(y_age_groups_vf)\n",
    "y_species_vf = np.asarray(y_species_vf)\n",
    "y_status_vf = np.asarray(y_status_vf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Running\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 1625, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv1D)                  (None, 1618, 16)     144         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_1 (BatchNormalization (None, 1618, 16)     64          Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 1618, 16)     0           batchnorm_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv2 (Conv1D)                  (None, 806, 16)      2064        max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_2 (BatchNormalization (None, 806, 16)      64          Conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 806, 16)      0           batchnorm_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv3 (Conv1D)                  (None, 804, 16)      784         max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_3 (BatchNormalization (None, 804, 16)      64          Conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 804, 16)      0           batchnorm_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv4 (Conv1D)                  (None, 400, 16)      1552        max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_4 (BatchNormalization (None, 400, 16)      64          Conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 400, 16)      0           batchnorm_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv5 (Conv1D)                  (None, 396, 16)      1296        max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_5 (BatchNormalization (None, 396, 16)      64          Conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 198, 16)      0           batchnorm_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 3168)         0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dout6 (Dropout)                 (None, 3168)         0           flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "d6 (Dense)                      (None, 500)          1584500     dout6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_6 (BatchNormalization (None, 500)          2000        d6[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "age_group (Dense)               (None, 3)            1503        batchnorm_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "species (Dense)                 (None, 3)            1503        batchnorm_6[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,595,666\n",
      "Trainable params: 1,594,506\n",
      "Non-trainable params: 1,160\n",
      "__________________________________________________________________________________________________\n",
      "696/696 [==============================] - 0s 156us/step\n",
      "Fold 1 Running\n",
      "696/696 [==============================] - 0s 163us/step\n",
      "Fold 2 Running\n",
      "696/696 [==============================] - 0s 354us/step\n",
      "Fold 3 Running\n",
      "696/696 [==============================] - 0s 301us/step\n",
      "Fold 4 Running\n",
      "696/696 [==============================] - 0s 76us/step\n",
      "Fold 5 Running\n",
      "696/696 [==============================] - 0s 78us/step\n",
      "Fold 6 Running\n",
      "696/696 [==============================] - 0s 76us/step\n",
      "Fold 7 Running\n",
      "696/696 [==============================] - 0s 79us/step\n",
      "Fold 8 Running\n",
      "696/696 [==============================] - 0s 78us/step\n",
      "Fold 9 Running\n",
      "696/696 [==============================] - 0s 82us/step\n",
      "Run time : 29554.336055994034 s\n",
      "Run time : 492.57226759990056 m\n",
      "Run time : 8.209537793331677 h\n"
     ]
    }
   ],
   "source": [
    "input_layer_dim = len(Xf[0])\n",
    "\n",
    "y_age_groups_list_l = [[age] for age in y_age_groups]\n",
    "y_species_list_l = [[species] for species in y_species]\n",
    "age_groups_l = MultiLabelBinarizer().fit_transform(np.array(y_age_groups_list_l))\n",
    "age_group_classes = [\"1-4\", \"5-10\", \"11-17\"]\n",
    "species_l = MultiLabelBinarizer().fit_transform(np.array(y_species_list_l))\n",
    "species_classes = list(np.unique(y_species_list_l))\n",
    "\n",
    "y_age_groups_list_f = [[age] for age in y_age_groupsf]\n",
    "y_species_list_f = [[species] for species in y_speciesf]\n",
    "age_groups_f = MultiLabelBinarizer().fit_transform(np.array(y_age_groups_list_f))\n",
    "species_f = MultiLabelBinarizer().fit_transform(np.array(y_species_list_f))\n",
    "\n",
    "outdir = \"Results_Paper/\"\n",
    "build_folder(outdir, False)\n",
    "SelectFreqs = False\n",
    "  \n",
    "## Labels default - all classification\n",
    "labels_default_f, labels_default_l, classes_default, outputs_default = [age_groups_f, species_f], [age_groups_l, species_l], [age_group_classes, species_classes], ['xAgeGroup', 'xSpecies']\n",
    "\n",
    "\n",
    "## Declare and train the model\n",
    "model_size = [{'type':'c', 'filter':16, 'kernel':8, 'stride':1, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':8, 'stride':2, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':3, 'stride':1, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':6, 'stride':2, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':5, 'stride':1, 'pooling':2},\n",
    "             {'type':'d', 'width':500}]\n",
    "\n",
    "## Name the model\n",
    "model_name = 'Baseline_CNN'\n",
    "    \n",
    "histories = []\n",
    "fold = 1\n",
    "train_model = True\n",
    "\n",
    "## Name a folder for the outputs to go into\n",
    "savedir = (outdir+\"Trian_Lab_Field_V2/\")\n",
    "build_folder(savedir, True)\n",
    "    \n",
    "start_time = time()\n",
    "save_predicted = []\n",
    "save_true = []\n",
    "\n",
    "## Scale train, test\n",
    "scl = StandardScaler()\n",
    "features_scl = scl.fit(X=np.vstack((X,Xf)))\n",
    "features_l = features_scl.transform(X=X)\n",
    "features_f = features_scl.transform(X=Xf)\n",
    "\n",
    "## Split into training / testing\n",
    "test_splits = train_test_split(features_f, *(labels_default_f), test_size=0.1, shuffle=True, random_state=rand_seed)\n",
    "## Pack up data\n",
    "X_train = test_splits.pop(0)\n",
    "X_test = test_splits.pop(0)\n",
    "y_train = test_splits[::2]\n",
    "y_test = test_splits[1::2]\n",
    "\n",
    "X_train = np.vstack((X_train, features_l))\n",
    "y_train = [np.vstack((y_train[0], labels_default_l[0])), np.vstack((y_train[1], labels_default_l[1]))]\n",
    "\n",
    "\n",
    "if not SelectFreqs:\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "\n",
    "model_to_test = {\n",
    "    \"model_shape\" : [model_size], # defines the hidden layers of the model\n",
    "    \"model_name\"  : [model_name],\n",
    "    \"input_layer_dim\"  : [input_layer_dim], # size of input layer\n",
    "    \"model_ver_num\"  : [0],\n",
    "    \"fold\"  : [fold], # kf.split number on\n",
    "    \"labels\"   : [y_train],\n",
    "    \"features\" : [X_train],\n",
    "    \"classes\"  : [classes_default],\n",
    "    \"outputs\"   : [outputs_default],\n",
    "    \"compile_loss\": [{'age': 'categorical_crossentropy'}],\n",
    "    \"compile_metrics\" :[{'age': 'accuracy'}]\n",
    "}\n",
    "\n",
    "## Call function to train all the models from the dictionary\n",
    "model, history = train_models(model_to_test, savedir, SelectFreqs=SelectFreqs)\n",
    "histories.append(history)\n",
    "\n",
    "predicted_labels = list([] for i in range(len(y_train)))\n",
    "true_labels = list([] for i in range(len(y_train)))\n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "predicted_labels = [x+[y] for x,y in zip(predicted_labels,y_predicted)]\n",
    "true_labels = [x+[y] for x,y in zip(true_labels,y_test)]\n",
    "\n",
    "predicted_labels = [predicted_labels[i][0].tolist() for i in range(len(predicted_labels))]\n",
    "true_labels = [true_labels[i][0].tolist() for i in range(len(true_labels))]\n",
    "\n",
    "for pred, tru in zip(predicted_labels, true_labels):\n",
    "    save_predicted.append(pred)\n",
    "    save_true.append(tru)\n",
    "\n",
    "## Visualize the results\n",
    "visualize(histories, savedir, model_name, str(fold), classes_default, outputs_default, predicted_labels, true_labels)\n",
    "# log_data(test_index, 'test_index', fold, savedir)\n",
    "\n",
    "# Clear the Keras session, otherwise it will keep adding new\n",
    "# models to the same TensorFlow graph each time we create\n",
    "# a model with a different set of hyper-parameters.\n",
    "K.clear_session()\n",
    "\n",
    "# Delete the Keras model with these hyper-parameters from memory.\n",
    "del model\n",
    "\n",
    "# visualize(1, savedir, model_name, \"Averaged\", classes_default, outputs_default, save_predicted, save_true)\n",
    "end_time = time()\n",
    "print('Run time : {} s'.format(end_time-start_time))\n",
    "print('Run time : {} m'.format((end_time-start_time)/60))\n",
    "print('Run time : {} h'.format((end_time-start_time)/3600))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Phase\n",
    "### Process testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "input_layer_dim = len(Xf[0])\n",
    "\n",
    "y_age_groups_list_l = [[age] for age in y_age_groups]\n",
    "y_species_list_l = [[species] for species in y_species]\n",
    "age_groups_l = MultiLabelBinarizer().fit_transform(np.array(y_age_groups_list_l))\n",
    "age_group_classes = [\"1-4\", \"5-10\", \"11-17\"]\n",
    "species_l = MultiLabelBinarizer().fit_transform(np.array(y_species_list_l))\n",
    "species_classes = list(np.unique(y_species_list_l))\n",
    "\n",
    "y_age_groups_list_f = [[age] for age in y_age_groupsf]\n",
    "y_species_list_f = [[species] for species in y_speciesf]\n",
    "age_groups_f = MultiLabelBinarizer().fit_transform(np.array(y_age_groups_list_f))\n",
    "species_f = MultiLabelBinarizer().fit_transform(np.array(y_species_list_f))\n",
    "\n",
    "outdir = \"Results_Paper/\"\n",
    "build_folder(outdir, False)\n",
    "SelectFreqs = False\n",
    "  \n",
    "## Labels default - all classification\n",
    "labels_default_f, labels_default_l, classes_default, outputs_default = [age_groups_f, species_f], [age_groups_l, species_l], [age_group_classes, species_classes], ['xAgeGroup', 'xSpecies']\n",
    "\n",
    "## Name the model\n",
    "model_name = 'Baseline_CNN'\n",
    "    \n",
    "histories = []\n",
    "fold = 1\n",
    "train_model = False\n",
    "\n",
    "## Name a folder for the outputs to go into\n",
    "savedir = (outdir+\"Trian_Lab_Field_V2/\")\n",
    "build_folder(savedir, False)\n",
    "    \n",
    "start_time = time()\n",
    "save_predicted = []\n",
    "save_true = []\n",
    "\n",
    "## Scale train, test\n",
    "scl = StandardScaler()\n",
    "features_scl = scl.fit(X=np.vstack((X,Xf)))\n",
    "features_l = features_scl.transform(X=X)\n",
    "features_f = features_scl.transform(X=Xf)\n",
    "\n",
    "## Split into training / testing\n",
    "test_splits = train_test_split(features_f, *(labels_default_f), test_size=0.1, shuffle=True, random_state=rand_seed)\n",
    "## Pack up data\n",
    "X_train = test_splits.pop(0)\n",
    "X_test = test_splits.pop(0)\n",
    "y_train = test_splits[::2]\n",
    "y_test = test_splits[1::2]\n",
    "\n",
    "X_train = np.vstack((X_train, features_l))\n",
    "y_train = [np.vstack((y_train[0], labels_default_l[0])), np.vstack((y_train[1], labels_default_l[1]))]\n",
    "\n",
    "\n",
    "if not SelectFreqs:\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "\n",
    "model = load_model((savedir+\"Baseline_CNN_Model.h5\"))\n",
    "# model.summary()\n",
    "print('Model loaded successfully') \n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "predicted_labels = list([] for i in range(len(y_train)))\n",
    "true_labels = list([] for i in range(len(y_train)))\n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "predicted_labels = [x+[y] for x,y in zip(predicted_labels,y_predicted)]\n",
    "true_labels = [x+[y] for x,y in zip(true_labels,y_test)]\n",
    "\n",
    "predicted_labels = [predicted_labels[i][0].tolist() for i in range(len(predicted_labels))]\n",
    "true_labels = [true_labels[i][0].tolist() for i in range(len(true_labels))]\n",
    "\n",
    "for pred, tru in zip(predicted_labels, true_labels):\n",
    "    save_predicted.append(pred)\n",
    "    save_true.append(tru)\n",
    "\n",
    "## Visualize the results\n",
    "visualize(histories, savedir, model_name, str(fold), classes_default, outputs_default, predicted_labels, true_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
