{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook provides the functionality to build, train, and test a CNN for predicting mosquito age, grouped age, species, and status.\n",
    "\n",
    "## Structure:\n",
    "* Import packages to be used.\n",
    "* Load mosquito data.\n",
    "* Define fucntions for plotting, visualisation, and logging.\n",
    "* Define a function to build the CNN.\n",
    "* Define a function to train the CNN.\n",
    "* Main section to organise data, define the CNN, and call the building and training of the CNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/anaconda3/envs/mosquitoes/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/josh/anaconda3/envs/mosquitoes/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/josh/anaconda3/envs/mosquitoes/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/josh/anaconda3/envs/mosquitoes/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/josh/anaconda3/envs/mosquitoes/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/josh/anaconda3/envs/mosquitoes/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pylab as pl\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "import pickle\n",
    "import random as rn\n",
    "import os\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib\n",
    "\n",
    "font = {'weight' : 'normal',\n",
    "        'size'   : 18}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras import layers, metrics\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.models import model_from_json, load_model\n",
    "from keras.regularizers import *\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# rand_seed = np.random.randint(low=0, high=100)\n",
    "rand_seed = 16\n",
    "print(rand_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "## The below is necessary for starting Numpy generated random numbers in a well-defined initial state.\n",
    "np.random.seed(rand_seed)\n",
    "\n",
    "## The below is necessary for starting core Python generated random numbers in a well-defined state.\n",
    "rn.seed(12345)\n",
    "\n",
    "## Force TensorFlow to use single thread.\n",
    "## Multiple threads are a potential source of\n",
    "## non-reproducible results.\n",
    "## For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "# session_conf = tf.ConfigProto(device_count = {'GPU':0}, intra_op_parallelism_threads=4) #session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "# session_conf = tf.ConfigProto(device_count = {'GPU':0}) #session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "#session_conf.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "## The below tf.set_random_seed() will make random number generation\n",
    "## in the TensorFlow backend have a well-defined initial state.\n",
    "## For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.35)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "The data file is created using Loco Mosquito:\n",
    "https://github.com/magonji/MIMI-project/blob/master/Loco%20mosquito%204.0.ipynb\n",
    "\n",
    "### The data file has headings: Species - Status - RearCnd - Age - Country- Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moved to each model run section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function used to create a new folder for the CNN outputs.\n",
    "Useful to stop forgetting to name a new folder when trying out a new model varient and overwriting a days training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_folder(fold, to_build = False):\n",
    "    if not os.path.isdir(fold):\n",
    "        if to_build == True:\n",
    "            os.mkdir(fold)\n",
    "        else:\n",
    "            print('Directory does not exists, not creating directory!')\n",
    "    else:\n",
    "        if to_build == True:\n",
    "            raise NameError('Directory already exists, cannot be created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for plotting confusion matrcies\n",
    "This normalizes the confusion matrix and ensures neat plotting for all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, output, save_path, model_name, fold,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          printout=False):\n",
    "\n",
    "    font = {'weight' : 'normal',\n",
    "            'size'   : 15}\n",
    "\n",
    "    matplotlib.rc('font', **font)\n",
    "\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        if printout:\n",
    "            print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        if printout:\n",
    "            print('Confusion matrix, without normalization')\n",
    "\n",
    "    if printout:\n",
    "        print(cm)\n",
    "    \n",
    "    plt.figure(figsize=(2.8,2.8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, vmin=0, vmax=1) # np.max(np.sum(cm, axis=1)))\n",
    "#     plt.title([title+' - '+model_name])\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.ylim(2.5,-0.5)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout(pad=0.3)\n",
    "#     plt.tight_layout()\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')\n",
    "    plt.savefig((save_path+\"Confusion_Matrix_\"+model_name+\"_\"+fold+\"_\"+output[1:]+\".pdf\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function used for visualizing outputs\n",
    "This splits the output data into the four categories before plotting the confusion matricies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for visualizing losses and metrics once the neural network fold is trained\n",
    "def visualize(histories, save_path, model_name, fold, classes, outputs, predicted, true):\n",
    "    # Sort out predictions and true labels\n",
    "    for label_predictions_arr, label_true_arr, classes, outputs in zip(predicted, true, classes, outputs):\n",
    "        classes_pred = np.argmax(label_predictions_arr, axis=-1)\n",
    "        classes_true = np.argmax(label_true_arr, axis=-1)\n",
    "        cnf_matrix = confusion_matrix(classes_true, classes_pred)\n",
    "        plot_confusion_matrix(cnf_matrix, classes, outputs, save_path, model_name, fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for logging data associated with the model\n",
    "def log_data(log, name, fold, save_path):\n",
    "    f = open((save_path+name+'_'+str(fold)+'_log.txt'), 'w')\n",
    "    np.savetxt(f, log)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fucntion for graphing the training data\n",
    "This fucntion creates tidy graphs of loss and accuracy as the models are training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_history(history, model_name, model_ver_num, fold, save_path):\n",
    "\n",
    "    font = {'weight' : 'normal',\n",
    "            'size'   : 18}\n",
    "\n",
    "    matplotlib.rc('font', **font)\n",
    "    \n",
    "    #not_validation = list(filter(lambda x: x[0:3] != \"val\", history.history.keys()))\n",
    "#     print('history.history.keys : {}'.format(history.history.keys()))\n",
    "    filtered = filter(lambda x: x[0:3] != \"val\", history.history.keys())\n",
    "    not_validation = list(filtered)\n",
    "    for i in not_validation:\n",
    "        plt.figure(figsize=(15,7))\n",
    "#         plt.title(i+\"/ \"+\"val_\"+i)\n",
    "        plt.plot(history.history[i], label=i)\n",
    "        plt.plot(history.history[\"val_\"+i], label=\"val_\"+i)\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(i)\n",
    "        plt.savefig(save_path +model_name+\"_\"+str(model_ver_num)+\"_\"+str(fold)+\"_\"+i)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funciton to create the CNN\n",
    "This function takes as an input a list of dictionaries. Each element in the list is a new hidden layer in the model. For each layer the dictionary defines the layer to be used.\n",
    "\n",
    "### Available options are:\n",
    "Convolutional Layer:\n",
    "* type = 'c'\n",
    "* filter = optional number of filters\n",
    "* kernel = optional size of the filters\n",
    "* stride = optional size of stride to take between filters\n",
    "* pooling = optional width of the max pooling\n",
    "* {'type':'c', 'filter':16, 'kernel':5, 'stride':1, 'pooling':2}\n",
    "\n",
    "dense layer:\n",
    "* type = 'd'\n",
    "* width = option width of the layer\n",
    "* {'type':'d', 'width':500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(model_shape, input_layer_dim, SelectFreqs):\n",
    "\n",
    "    regConst = 0.02\n",
    "    sgd = keras.optimizers.SGD(lr=0.003, decay=1e-5, momentum=0.9, nesterov=True, clipnorm=1.)\n",
    "    cce = 'categorical_crossentropy'\n",
    "\n",
    "    if SelectFreqs:\n",
    "        input_vec = Input(name='input', shape=(input_layer_dim,))\n",
    "        xd = Dense(name='d1', units=500, activation='relu', \n",
    "                     kernel_regularizer=l2(regConst), \n",
    "                     kernel_initializer='he_normal')(input_vec)\n",
    "        xd = BatchNormalization(name='batchnorm_1')(xd)\n",
    "        \n",
    "    else:\n",
    "        input_vec = Input(name='input', shape=(input_layer_dim,1))\n",
    "        \n",
    "        for i, layerwidth in zip(range(len(model_shape)),model_shape):\n",
    "            if i == 0:\n",
    "                if model_shape[i]['type'] == 'c':\n",
    "                    xd = Conv1D(name=('Conv'+str(i+1)), filters=model_shape[i]['filter'], \n",
    "                     kernel_size = model_shape[i]['kernel'], strides = model_shape[i]['stride'],\n",
    "                     activation = 'relu',\n",
    "                     kernel_regularizer=l2(regConst), \n",
    "                     kernel_initializer='he_normal')(input_vec)\n",
    "                    xd = BatchNormalization(name=('batchnorm_'+str(i+1)))(xd)\n",
    "                    xd = MaxPooling1D(pool_size=(model_shape[i]['pooling']))(xd)\n",
    "\n",
    "                elif model_shape[i]['type'] == 'd':\n",
    "                    xd = Dense(name=('d'+str(i+1)), units=model_shape[i]['width'], activation='relu', \n",
    "                     kernel_regularizer=l2(regConst), \n",
    "                     kernel_initializer='he_normal')(input_vec)\n",
    "                    xd = BatchNormalization(name=('batchnorm_'+str(i+1)))(xd) \n",
    "                    xd = Dropout(name=('dout'+str(i+1)), rate=0.5)(xd) \n",
    "\n",
    "            else:\n",
    "                if model_shape[i]['type'] == 'c':\n",
    "                    xd = Conv1D(name=('Conv'+str(i+1)), filters=model_shape[i]['filter'], \n",
    "                     kernel_size = model_shape[i]['kernel'], strides = model_shape[i]['stride'],\n",
    "                     activation = 'relu',\n",
    "                     kernel_regularizer=l2(regConst), \n",
    "                     kernel_initializer='he_normal')(xd)\n",
    "                    xd = BatchNormalization(name=('batchnorm_'+str(i+1)))(xd)\n",
    "                    xd = MaxPooling1D(pool_size=(model_shape[i]['pooling']))(xd)\n",
    "\n",
    "                elif model_shape[i]['type'] == 'd':\n",
    "                    if model_shape[i-1]['type'] == 'c':\n",
    "                        xd = Flatten()(xd)\n",
    "\n",
    "                    xd = Dropout(name=('dout'+str(i+1)), rate=0.5)(xd)\n",
    "                    xd = Dense(name=('d'+str(i+1)), units=model_shape[i]['width'], activation='relu', \n",
    "                     kernel_regularizer=l2(regConst), \n",
    "                     kernel_initializer='he_normal')(xd)\n",
    "                    xd = BatchNormalization(name=('batchnorm_'+str(i+1)))(xd) \n",
    "        \n",
    "    \n",
    "#     xAge     = Dense(name = 'age', units = 17, \n",
    "#                      activation = 'softmax', \n",
    "#                      kernel_regularizer = l2(regConst), \n",
    "#                      kernel_initializer = 'he_normal')(xd)\n",
    "    xAgeGroup     = Dense(name = 'age_group', units = 3, \n",
    "                     activation = 'softmax', \n",
    "                     kernel_regularizer = l2(regConst), \n",
    "                     kernel_initializer = 'he_normal')(xd)\n",
    "    xSpecies = Dense(name ='species', units = 3, \n",
    "                     activation = 'softmax', \n",
    "                     kernel_regularizer = l2(regConst), \n",
    "                     kernel_initializer = 'he_normal')(xd)\n",
    "\n",
    "    outputs = []\n",
    "#     for i in ['xAge', 'xAgeGroup', 'xSpecies']:\n",
    "    for i in ['xAgeGroup', 'xSpecies']:\n",
    "        outputs.append(locals()[i])\n",
    "    model = Model(inputs = input_vec, outputs = outputs)\n",
    "    \n",
    "    model.compile(loss=cce, metrics=['acc'], \n",
    "                  optimizer=sgd)\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to train the model\n",
    "\n",
    "This function will split the data into training and validation and call the create models function. This fucntion returns the model and training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(model_to_test, save_path, SelectFreqs=False):\n",
    "    \n",
    "    out_path = save_path+'out/'\n",
    "    build_folder(out_path, True)\n",
    "\n",
    "    model_shape = model_to_test[\"model_shape\"][0]\n",
    "    model_name = model_to_test[\"model_name\"][0]\n",
    "#     input_layer_dim = model_to_test[\"input_layer_dim\"][0]\n",
    "    model_ver_num = model_to_test[\"model_ver_num\"][0]\n",
    "    fold = model_to_test[\"fold\"][0]\n",
    "    label = model_to_test[\"labels\"][0]\n",
    "    features = model_to_test[\"features\"][0]\n",
    "    classes = model_to_test[\"classes\"][0]\n",
    "    outputs = model_to_test[\"outputs\"][0]\n",
    "    compile_loss = model_to_test[\"compile_loss\"][0]\n",
    "    compile_metrics = model_to_test[\"compile_metrics\"][0]\n",
    "\n",
    "#     ## Split into training / testing\n",
    "#     test_splits = train_test_split(features, *(label), test_size=0.1, shuffle=True, random_state=rand_seed)\n",
    "#     ## Pack up data\n",
    "#     X_train = test_splits.pop(0)\n",
    "#     X_val = test_splits.pop(0)\n",
    "#     y_train = test_splits[::2]\n",
    "#     y_val = test_splits[1::2]\n",
    "    \n",
    "#     out_model = create_models(model_shape, input_layer_dim, SelectFreqs)\n",
    "#     out_model.summary()\n",
    "#     out_history = out_model.fit(x = X_train, \n",
    "#                             y = y_train,\n",
    "#                             batch_size = 128*16, \n",
    "#                             verbose = 0, \n",
    "#                             epochs = 8000,\n",
    "#                             validation_data = (X_val, y_val),\n",
    "#                             callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "#                                         patience=400, verbose=0, mode='auto'), \n",
    "#                                         CSVLogger(save_path+model_name+\"_\"+str(model_ver_num)+'.csv', append=True, separator=';')])\n",
    "#     scores = out_model.evaluate(X_val, y_val)\n",
    "#     print(out_model.metrics_names)\n",
    "\n",
    "    ## Kfold training\n",
    "    seed = rand_seed\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "    ## Split data into test and train\n",
    "    \n",
    "    model_ver_num = 0\n",
    "    cv_scores = []\n",
    "    best_score = 0\n",
    "    for train_index, val_index in kfold.split(features):\n",
    "        print('Fold {} Running'.format(model_ver_num))\n",
    "        \n",
    "        X_train, X_val = features[train_index], features[val_index]\n",
    "        y_train, y_val = list(map(lambda y:y[train_index], label)), list(map(lambda y:y[val_index], label))\n",
    "\n",
    "        model = create_models(model_shape, input_layer_dim, SelectFreqs)\n",
    "        if model_ver_num == 0:\n",
    "            model.summary()\n",
    "\n",
    "        history = model.fit(x = X_train, \n",
    "                            y = y_train,\n",
    "                            batch_size = 128*16, \n",
    "                            verbose = 0, \n",
    "                            epochs = 8000,\n",
    "                            validation_data = (X_val, y_val),\n",
    "                            callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                        patience=400, verbose=0, mode='auto'), \n",
    "                                        CSVLogger(out_path+model_name+\"_\"+str(model_ver_num)+'.csv', append=True, separator=';')])\n",
    "        scores = model.evaluate(X_val, y_val)\n",
    "        model.save((out_path+model_name+\"_\"+str(model_ver_num)+'_Model.h5'))\n",
    "        graph_history(history, model_name, model_ver_num, 0, out_path)\n",
    "#         print(model.metrics_names)\n",
    "#         print(scores)\n",
    "        if (scores[3] + scores[4]) > best_score:\n",
    "            out_model = model\n",
    "            out_history = history\n",
    "        \n",
    "        model_ver_num += 1\n",
    "        \n",
    "#         # Clear the Keras session, otherwise it will keep adding new\n",
    "#         # models to the same TensorFlow graph each time we create\n",
    "#         # a model with a different set of hyper-parameters.\n",
    "#         K.clear_session()\n",
    "\n",
    "#         # Delete the Keras model with these hyper-parameters from memory.\n",
    "#         del model\n",
    "        \n",
    "    out_model.save((save_path+'Best_Model.h5'))\n",
    "    graph_history(out_history, 'Best_Model', 0, 0, save_path)\n",
    "    \n",
    "    return out_model, out_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main section\n",
    "\n",
    "Functionality:\n",
    "* Oganises the data into a format of lists of data, classes, labels.\n",
    "* Define the CNN to be built.\n",
    "* Define the KFold validation to be used.\n",
    "* Build a folder to output data into.\n",
    "* Standardize and oragnise data into training/testing.\n",
    "* Call the model training.\n",
    "* Organize outputs and call visualization for plotting and graphing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = \"Results_Paper/\"\n",
    "build_folder(outdir, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Bobo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RearCnd\n",
      "TF     9618\n",
      "TL    27838\n",
      "VF     3285\n",
      "dtype: int64\n",
      "df_l_g_a : AgeGroup\n",
      "0    306\n",
      "1    592\n",
      "2    763\n",
      "dtype: int64\n",
      "df_l_g_g : AgeGroup\n",
      "0    629\n",
      "1    827\n",
      "2    821\n",
      "dtype: int64\n",
      "df_l_g_c : AgeGroup\n",
      "0     318\n",
      "1     892\n",
      "2    1282\n",
      "dtype: int64\n",
      "df_l_t_a : AgeGroup\n",
      "0     708\n",
      "1    3474\n",
      "2    4375\n",
      "dtype: int64\n",
      "df_l_t_g : AgeGroup\n",
      "0     726\n",
      "1    3390\n",
      "2    4488\n",
      "dtype: int64\n",
      "df_l_b_g : AgeGroup\n",
      "0    481\n",
      "1    666\n",
      "2    858\n",
      "dtype: int64\n",
      "df_l_b_c : AgeGroup\n",
      "0    514\n",
      "1    777\n",
      "2    937\n",
      "dtype: int64\n",
      "df_f_t_a : AgeGroup\n",
      "0     668\n",
      "1    2316\n",
      "2    2703\n",
      "dtype: int64\n",
      "df_f_b_g : AgeGroup\n",
      "0     831\n",
      "1    1003\n",
      "2    1070\n",
      "dtype: int64\n",
      "df_f_b_c : AgeGroup\n",
      "0    458\n",
      "2    569\n",
      "dtype: int64\n",
      "df_vf_t_a : AgeGroup\n",
      "0    207\n",
      "1    627\n",
      "2    512\n",
      "dtype: int64\n",
      "df_vf_t_g : AgeGroup\n",
      "0    209\n",
      "1    584\n",
      "2    625\n",
      "dtype: int64\n",
      "df_vf_b_g : AgeGroup\n",
      "0    113\n",
      "1    104\n",
      "2     48\n",
      "dtype: int64\n",
      "df_vf_b_c : AgeGroup\n",
      "0    119\n",
      "1    109\n",
      "2     20\n",
      "dtype: int64\n",
      "Species\n",
      "AA    8557\n",
      "AG    8604\n",
      "dtype: int64\n",
      "Species\n",
      "AC    2228\n",
      "AG    2005\n",
      "dtype: int64\n",
      "Species\n",
      "AA    5687\n",
      "dtype: int64\n",
      "Species\n",
      "AC    1027\n",
      "AG    2904\n",
      "dtype: int64\n",
      "Species\n",
      "AA    1346\n",
      "AG    1418\n",
      "dtype: int64\n",
      "Species\n",
      "AC    248\n",
      "AG    265\n",
      "dtype: int64\n",
      "Species\n",
      "AA    2306\n",
      "AC    1118\n",
      "AG    2400\n",
      "dtype: int64\n",
      "Status\n",
      "BF    1466\n",
      "GR    1099\n",
      "SF    3259\n",
      "dtype: int64\n",
      "RearCnd\n",
      "TL    5824\n",
      "dtype: int64\n",
      "AgeGroup\n",
      "0    1824\n",
      "1    2000\n",
      "2    2000\n",
      "dtype: int64\n",
      "Species\n",
      "AC    2228\n",
      "AG    2005\n",
      "dtype: int64\n",
      "Status\n",
      "BF    1277\n",
      "GR    1138\n",
      "SF    1818\n",
      "dtype: int64\n",
      "RearCnd\n",
      "TL    4233\n",
      "dtype: int64\n",
      "AgeGroup\n",
      "0     995\n",
      "1    1443\n",
      "2    1795\n",
      "dtype: int64\n",
      "shape of X : (5824, 1625)\n",
      "shape of y age : (5824,)\n",
      "shape of y age groups : (5824,)\n",
      "shape of y species : (5824,)\n",
      "shape of y status : (5824,)\n",
      "shape of X_vf : (4233, 1625)\n",
      "shape of y_age_vf age : (4233,)\n",
      "shape of y_age_groups_vf : (4233,)\n",
      "shape of y y_species_vf : (4233,)\n",
      "shape of y y_status_vf : (4233,)\n"
     ]
    }
   ],
   "source": [
    "SelectFreqs = False\n",
    "\n",
    "df = pd.read_csv(\"../../..//Data/mosquitoes_country_LM_5_0.dat\", '\\t')\n",
    "df.head(10)\n",
    "RearCnd_counts = df.groupby('RearCnd').size()\n",
    "print('{}'.format(RearCnd_counts))\n",
    "\n",
    "df['AgeGroup'] = 0\n",
    "df['AgeGroup'] = np.where(df['Age']>10, 2, np.where(df['Age']>4, 1, 0))\n",
    "\n",
    "df_vf = df[df['RearCnd']=='VF']\n",
    "df_vf = df_vf[df_vf['Status']=='UN']\n",
    "df = df[df['RearCnd']!='VF']\n",
    "df = df[df['Status']!='UN']\n",
    "df_l = df[df['RearCnd']=='TL']\n",
    "df_l_g = df_l[df_l['Country']=='S']\n",
    "df_l_g_a = df_l_g[df_l_g['Species']=='AA']\n",
    "age_counts = df_l_g_a.groupby('AgeGroup').size()\n",
    "print('df_l_g_a : {}'.format(age_counts))\n",
    "df_l_g_g = df_l_g[df_l_g['Species']=='AG']\n",
    "age_counts = df_l_g_g.groupby('AgeGroup').size()\n",
    "print('df_l_g_g : {}'.format(age_counts))\n",
    "df_l_g_c = df_l_g[df_l_g['Species']=='AC']\n",
    "age_counts = df_l_g_c.groupby('AgeGroup').size()\n",
    "print('df_l_g_c : {}'.format(age_counts))\n",
    "df_l_t = df_l[df_l['Country']=='T']\n",
    "df_l_t_a = df_l_t[df_l_t['Species']=='AA']\n",
    "age_counts = df_l_t_a.groupby('AgeGroup').size()\n",
    "print('df_l_t_a : {}'.format(age_counts))\n",
    "df_l_t_g = df_l_t[df_l_t['Species']=='AG']\n",
    "age_counts = df_l_t_g.groupby('AgeGroup').size()\n",
    "print('df_l_t_g : {}'.format(age_counts))\n",
    "df_l_b = df_l[df_l['Country']=='B']\n",
    "df_l_b_g = df_l_b[df_l_b['Species']=='AG']\n",
    "age_counts = df_l_b_g.groupby('AgeGroup').size()\n",
    "print('df_l_b_g : {}'.format(age_counts))\n",
    "df_l_b_c = df_l_b[df_l_b['Species']=='AC']\n",
    "age_counts = df_l_b_c.groupby('AgeGroup').size()\n",
    "print('df_l_b_c : {}'.format(age_counts))\n",
    "df_f = df[df['RearCnd']=='TF']\n",
    "df_f_t = df_f[df_f['Country']=='T']\n",
    "df_f_t_a = df_f_t[df_f_t['Species']=='AA']\n",
    "age_counts = df_f_t_a.groupby('AgeGroup').size()\n",
    "print('df_f_t_a : {}'.format(age_counts))\n",
    "# df_f_t_g = df_f_t[df_f_t['Species']=='AG'] #There isn't any\n",
    "df_f_b = df_f[df_f['Country']=='B']\n",
    "df_f_b_g = df_f_b[df_f_b['Species']=='AG']\n",
    "age_counts = df_f_b_g.groupby('AgeGroup').size()\n",
    "print('df_f_b_g : {}'.format(age_counts))\n",
    "df_f_b_c = df_f_b[df_f_b['Species']=='AC']\n",
    "age_counts = df_f_b_c.groupby('AgeGroup').size()\n",
    "print('df_f_b_c : {}'.format(age_counts))\n",
    "df_vf_t = df_vf[df_vf['Country']=='T']\n",
    "df_vf_t_a = df_vf_t[df_vf_t['Species']=='AA']\n",
    "age_counts = df_vf_t_a.groupby('AgeGroup').size()\n",
    "print('df_vf_t_a : {}'.format(age_counts))\n",
    "df_vf_t_g = df_vf_t[df_vf_t['Species']=='AG']\n",
    "age_counts = df_vf_t_g.groupby('AgeGroup').size()\n",
    "print('df_vf_t_g : {}'.format(age_counts))\n",
    "df_vf_b = df_vf[df_vf['Country']=='B']\n",
    "df_vf_b_g = df_vf_b[df_vf_b['Species']=='AG']\n",
    "age_counts = df_vf_b_g.groupby('AgeGroup').size()\n",
    "print('df_vf_b_g : {}'.format(age_counts))\n",
    "df_vf_b_c = df_vf_b[df_vf_b['Species']=='AC']\n",
    "age_counts = df_vf_b_c.groupby('AgeGroup').size()\n",
    "print('df_vf_b_c : {}'.format(age_counts))\n",
    "Species_counts = df_l_t.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_l_b.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_f_t.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_f_b.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_vf_t.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_vf_b.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "\n",
    "size_inc = 400\n",
    "for age in range(3):\n",
    "    df_temp = df_l_t_a[df_l_t_a['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    if age == 0:\n",
    "        df_train = df_temp.iloc[index_df_temp_inc]\n",
    "#         df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "    else:\n",
    "        df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#         df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_t_g[df_l_t_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 0\n",
    "for age in range(3):\n",
    "    df_temp = df_l_b_g[df_l_b_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    if age == 0:\n",
    "        df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "    else:\n",
    "#         df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "        df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_b_c[df_l_b_c['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "#     df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 400\n",
    "for age in range(3):\n",
    "    df_temp = df_l_g_a[df_l_g_a['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_g_g[df_l_g_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_g_c[df_l_g_c['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "\n",
    "    \n",
    "Species_counts = df_train.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Status_counts = df_train.groupby('Status').size()\n",
    "print('{}'.format(Status_counts))\n",
    "RearCnd_counts = df_train.groupby('RearCnd').size()\n",
    "print('{}'.format(RearCnd_counts))\n",
    "age_counts = df_train.groupby('AgeGroup').size()\n",
    "print('{}'.format(age_counts))\n",
    "Species_counts = df_test.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Status_counts = df_test.groupby('Status').size()\n",
    "print('{}'.format(Status_counts))\n",
    "RearCnd_counts = df_test.groupby('RearCnd').size()\n",
    "print('{}'.format(RearCnd_counts))\n",
    "age_counts = df_test.groupby('AgeGroup').size()\n",
    "print('{}'.format(age_counts))\n",
    "\n",
    "\n",
    "if SelectFreqs:\n",
    "    X = df_train[[\"3400\",\"3276\",\"2922\",\"2854\",\"1900\",\"1746\",\"1636\",\"1540\",\"1458\",\"1306\",\"1154\",\"1076\",\"1028\",\"880\"]]\n",
    "else:\n",
    "    X = df_train.iloc[:,6:-1]\n",
    "y_age = df_train[\"Age\"]\n",
    "y_age_groups = df_train[\"AgeGroup\"]\n",
    "y_species = df_train[\"Species\"]\n",
    "y_status = df_train[\"Status\"]\n",
    "\n",
    "print('shape of X : {}'.format(X.shape))\n",
    "print('shape of y age : {}'.format(y_age.shape))\n",
    "print('shape of y age groups : {}'.format(y_age_groups.shape))\n",
    "print('shape of y species : {}'.format(y_species.shape))\n",
    "print('shape of y status : {}'.format(y_status.shape))\n",
    "\n",
    "X = np.asarray(X)[:, :-1]\n",
    "y_age = np.asarray(y_age)\n",
    "y_age_groups = np.asarray(y_age_groups)\n",
    "y_species = np.asarray(y_species)\n",
    "y_status = np.asarray(y_status)\n",
    "\n",
    "if SelectFreqs:\n",
    "    X_vf = df_test[[\"3400\",\"3276\",\"2922\",\"2854\",\"1900\",\"1746\",\"1636\",\"1540\",\"1458\",\"1306\",\"1154\",\"1076\",\"1028\",\"880\"]]\n",
    "else:\n",
    "    X_vf = df_test.iloc[:,6:-1]\n",
    "y_age_vf = df_test[\"Age\"]\n",
    "y_age_groups_vf = df_test[\"AgeGroup\"]\n",
    "y_species_vf = df_test[\"Species\"]\n",
    "y_status_vf = df_test[\"Status\"]\n",
    "print('shape of X_vf : {}'.format(X_vf.shape))\n",
    "print('shape of y_age_vf age : {}'.format(y_age_vf.shape))\n",
    "print('shape of y_age_groups_vf : {}'.format(y_age_groups_vf.shape))\n",
    "print('shape of y y_species_vf : {}'.format(y_species_vf.shape))\n",
    "print('shape of y y_status_vf : {}'.format(y_status_vf.shape))\n",
    "X_vf = np.asarray(X_vf)[:,:-1]\n",
    "y_age_vf = np.asarray(y_age_vf)\n",
    "y_age_groups_vf = np.asarray(y_age_groups_vf)\n",
    "y_species_vf = np.asarray(y_species_vf)\n",
    "y_status_vf = np.asarray(y_status_vf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Running\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 1624, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv1D)                  (None, 1617, 16)     144         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_1 (BatchNormalization (None, 1617, 16)     64          Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1617, 16)     0           batchnorm_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv2 (Conv1D)                  (None, 805, 16)      2064        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_2 (BatchNormalization (None, 805, 16)      64          Conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 805, 16)      0           batchnorm_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv3 (Conv1D)                  (None, 803, 16)      784         max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_3 (BatchNormalization (None, 803, 16)      64          Conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 803, 16)      0           batchnorm_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv4 (Conv1D)                  (None, 399, 16)      1552        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_4 (BatchNormalization (None, 399, 16)      64          Conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 399, 16)      0           batchnorm_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv5 (Conv1D)                  (None, 395, 16)      1296        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_5 (BatchNormalization (None, 395, 16)      64          Conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 197, 16)      0           batchnorm_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3152)         0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dout6 (Dropout)                 (None, 3152)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "d6 (Dense)                      (None, 500)          1576500     dout6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_6 (BatchNormalization (None, 500)          2000        d6[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "age_group (Dense)               (None, 3)            1503        batchnorm_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "species (Dense)                 (None, 3)            1503        batchnorm_6[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,587,666\n",
      "Trainable params: 1,586,506\n",
      "Non-trainable params: 1,160\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b24b1c94536d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;31m## Call function to train all the models from the dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavedir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSelectFreqs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSelectFreqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a59ca9b41800>\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(model_to_test, save_path, SelectFreqs)\u001b[0m\n\u001b[1;32m     64\u001b[0m                             callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', \n\u001b[1;32m     65\u001b[0m                                         patience=400, verbose=0, mode='auto'), \n\u001b[0;32m---> 66\u001b[0;31m                                         CSVLogger(out_path+model_name+\"_\"+str(model_ver_num)+'.csv', append=True, separator=';')])\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ver_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_Model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mosquitoes/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/mosquitoes/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mosquitoes/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mosquitoes/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mosquitoes/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_layer_dim = len(X[0])\n",
    "\n",
    "## Train\n",
    "y_age_groups_list = [[age] for age in y_age_groups]\n",
    "y_species_list = [[species] for species in y_species]\n",
    "\n",
    "## Test\n",
    "y_age_groups_list_test = [[age] for age in y_age_groups_vf]\n",
    "y_species_list_test = [[species] for species in y_species_vf]\n",
    "\n",
    "age_mlb = MultiLabelBinarizer().fit(np.vstack((np.array(y_age_groups_list),np.array(y_age_groups_list_test))))\n",
    "age_groups = age_mlb.transform(np.array(y_age_groups_list))\n",
    "age_groups_test = age_mlb.transform(np.array(y_age_groups_list_test))\n",
    "\n",
    "species_mlb = MultiLabelBinarizer().fit(np.vstack((np.array(y_species_list),np.array(y_species_list_test))))\n",
    "species = species_mlb.transform(np.array(y_species_list))\n",
    "species_test = species_mlb.transform(np.array(y_species_list_test))\n",
    "\n",
    "age_group_classes = [\"1-4\", \"5-10\", \"11-17\"]\n",
    "species_classes = list(np.unique(np.vstack((np.array(y_species_list),np.array(y_species_list_test)))))\n",
    "\n",
    "## Labels default - all classification\n",
    "label, classes_default, outputs_default = [age_groups, species], [age_group_classes, species_classes], ['xAgeGroup', 'xSpecies']\n",
    "## Labels default - all classification\n",
    "label_test = [age_groups_test, species_test]\n",
    "\n",
    "outdir = \"Results_Paper/\"\n",
    "build_folder(outdir, False)\n",
    "SelectFreqs = False\n",
    "\n",
    "\n",
    "## Declare and train the model\n",
    "model_size = [{'type':'c', 'filter':16, 'kernel':8, 'stride':1, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':8, 'stride':2, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':3, 'stride':1, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':6, 'stride':2, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':5, 'stride':1, 'pooling':2},\n",
    "             {'type':'d', 'width':500}]\n",
    "\n",
    "## Name the model\n",
    "model_name = 'Baseline_CNN'    \n",
    "    \n",
    "histories = []\n",
    "fold = 1\n",
    "train_model = True\n",
    "\n",
    "## Name a folder for the outputs to go into\n",
    "savedir = (outdir+\"Trian_Lab_Only_No_Bobo/\")\n",
    "build_folder(savedir, True)\n",
    "    \n",
    "start_time = time()\n",
    "save_predicted = []\n",
    "save_true = []\n",
    "\n",
    "## Scale train, test\n",
    "scl = StandardScaler()\n",
    "features_scl = scl.fit(X=np.vstack((X, X_vf)))\n",
    "features = features_scl.transform(X=X)\n",
    "features_test = features_scl.transform(X=X_vf)\n",
    "\n",
    "## Split into training / testing\n",
    "# test_splits = train_test_split(features, *(label), test_size=0.1, shuffle=True, random_state=rand_seed)\n",
    "## Pack up data\n",
    "# X_train = test_splits.pop(0)\n",
    "# X_test = test_splits.pop(0)\n",
    "# y_train = test_splits[::2]\n",
    "# y_test = test_splits[1::2]\n",
    "X_train = features\n",
    "X_test = features_test\n",
    "y_train = label\n",
    "y_test = label_test\n",
    "\n",
    "if not SelectFreqs:\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "\n",
    "model_to_test = {\n",
    "    \"model_shape\" : [model_size], # defines the hidden layers of the model\n",
    "    \"model_name\"  : [model_name],\n",
    "    \"input_layer_dim\"  : [input_layer_dim], # size of input layer\n",
    "    \"model_ver_num\"  : [0],\n",
    "    \"fold\"  : [fold], # kf.split number on\n",
    "    \"labels\"   : [y_train],\n",
    "    \"features\" : [X_train],\n",
    "    \"classes\"  : [classes_default],\n",
    "    \"outputs\"   : [outputs_default],\n",
    "    \"compile_loss\": [{'age': 'categorical_crossentropy'}],\n",
    "    \"compile_metrics\" :[{'age': 'accuracy'}]\n",
    "}\n",
    "\n",
    "## Call function to train all the models from the dictionary\n",
    "model, history = train_models(model_to_test, savedir, SelectFreqs=SelectFreqs)\n",
    "histories.append(history)\n",
    "\n",
    "predicted_labels = list([] for i in range(len(y_train)))\n",
    "true_labels = list([] for i in range(len(y_train)))\n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "predicted_labels = [x+[y] for x,y in zip(predicted_labels,y_predicted)]\n",
    "true_labels = [x+[y] for x,y in zip(true_labels,y_test)]\n",
    "\n",
    "predicted_labels = [predicted_labels[i][0].tolist() for i in range(len(predicted_labels))]\n",
    "true_labels = [true_labels[i][0].tolist() for i in range(len(true_labels))]\n",
    "\n",
    "for pred, tru in zip(predicted_labels, true_labels):\n",
    "    save_predicted.append(pred)\n",
    "    save_true.append(tru)\n",
    "\n",
    "## Visualize the results\n",
    "visualize(histories, savedir, model_name, str(fold), classes_default, outputs_default, predicted_labels, true_labels)\n",
    "# log_data(test_index, 'test_index', fold, savedir)\n",
    "\n",
    "# Clear the Keras session, otherwise it will keep adding new\n",
    "# models to the same TensorFlow graph each time we create\n",
    "# a model with a different set of hyper-parameters.\n",
    "K.clear_session()\n",
    "\n",
    "# Delete the Keras model with these hyper-parameters from memory.\n",
    "del model\n",
    "\n",
    "# visualize(1, savedir, model_name, \"Averaged\", classes_default, outputs_default, save_predicted, save_true)\n",
    "end_time = time()\n",
    "print('Run time : {} s'.format(end_time-start_time))\n",
    "print('Run time : {} m'.format((end_time-start_time)/60))\n",
    "print('Run time : {} h'.format((end_time-start_time)/3600))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/.local/lib/python3.6/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "input_layer_dim = len(X[0])\n",
    "\n",
    "## Train\n",
    "y_age_groups_list = [[age] for age in y_age_groups]\n",
    "y_species_list = [[species] for species in y_species]\n",
    "\n",
    "## Test\n",
    "y_age_groups_list_test = [[age] for age in y_age_groups_vf]\n",
    "y_species_list_test = [[species] for species in y_species_vf]\n",
    "\n",
    "age_mlb = MultiLabelBinarizer().fit(np.vstack((np.array(y_age_groups_list),np.array(y_age_groups_list_test))))\n",
    "age_groups = age_mlb.transform(np.array(y_age_groups_list))\n",
    "age_groups_test = age_mlb.transform(np.array(y_age_groups_list_test))\n",
    "\n",
    "species_mlb = MultiLabelBinarizer().fit(np.vstack((np.array(y_species_list),np.array(y_species_list_test))))\n",
    "species = species_mlb.transform(np.array(y_species_list))\n",
    "species_test = species_mlb.transform(np.array(y_species_list_test))\n",
    "\n",
    "age_group_classes = [\"1-4\", \"5-10\", \"11-17\"]\n",
    "species_classes = list(np.unique(np.vstack((np.array(y_species_list),np.array(y_species_list_test)))))\n",
    "\n",
    "## Labels default - all classification\n",
    "label, classes_default, outputs_default = [age_groups, species], [age_group_classes, species_classes], ['xAgeGroup', 'xSpecies']\n",
    "## Labels default - all classification\n",
    "label_test = [age_groups_test, species_test]\n",
    "\n",
    "outdir = \"Results_Paper/\"\n",
    "build_folder(outdir, False)\n",
    "SelectFreqs = False\n",
    "\n",
    "## Name the model\n",
    "model_name = 'Baseline_CNN'    \n",
    "    \n",
    "histories = []\n",
    "fold = 1\n",
    "train_model = False\n",
    "\n",
    "## Name a folder for the outputs to go into\n",
    "savedir = (outdir+\"Trian_Lab_Only_No_Bobo/\")\n",
    "build_folder(savedir, False)\n",
    "    \n",
    "start_time = time()\n",
    "save_predicted = []\n",
    "save_true = []\n",
    "\n",
    "## Scale train, test\n",
    "scl = StandardScaler()\n",
    "features_scl = scl.fit(X=np.vstack((X, X_vf)))\n",
    "features = features_scl.transform(X=X)\n",
    "features_test = features_scl.transform(X=X_vf)\n",
    "\n",
    "## Split into training / testing\n",
    "# test_splits = train_test_split(features, *(label), test_size=0.1, shuffle=True, random_state=rand_seed)\n",
    "## Pack up data\n",
    "# X_train = test_splits.pop(0)\n",
    "# X_test = test_splits.pop(0)\n",
    "# y_train = test_splits[::2]\n",
    "# y_test = test_splits[1::2]\n",
    "X_train = features\n",
    "X_test = features_test\n",
    "y_train = label\n",
    "y_test = label_test\n",
    "\n",
    "if not SelectFreqs:\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "model = load_model((savedir+\"Best_Model.h5\"))\n",
    "print('Model loaded successfully') \n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "predicted_labels = list([] for i in range(len(y_train)))\n",
    "true_labels = list([] for i in range(len(y_train)))\n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "predicted_labels = [x+[y] for x,y in zip(predicted_labels,y_predicted)]\n",
    "true_labels = [x+[y] for x,y in zip(true_labels,y_test)]\n",
    "\n",
    "predicted_labels = [predicted_labels[i][0].tolist() for i in range(len(predicted_labels))]\n",
    "true_labels = [true_labels[i][0].tolist() for i in range(len(true_labels))]\n",
    "\n",
    "for pred, tru in zip(predicted_labels, true_labels):\n",
    "    save_predicted.append(pred)\n",
    "    save_true.append(tru)\n",
    "\n",
    "## Visualize the results\n",
    "visualize(histories, savedir, model_name, str(fold), classes_default, outputs_default, predicted_labels, true_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Tanzania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RearCnd\n",
      "TF     9618\n",
      "TL    27838\n",
      "VF     3285\n",
      "dtype: int64\n",
      "df_l_g_a : AgeGroup\n",
      "0    306\n",
      "1    592\n",
      "2    763\n",
      "dtype: int64\n",
      "df_l_g_g : AgeGroup\n",
      "0    629\n",
      "1    827\n",
      "2    821\n",
      "dtype: int64\n",
      "df_l_g_c : AgeGroup\n",
      "0     318\n",
      "1     892\n",
      "2    1282\n",
      "dtype: int64\n",
      "df_l_t_a : AgeGroup\n",
      "0     708\n",
      "1    3474\n",
      "2    4375\n",
      "dtype: int64\n",
      "df_l_t_g : AgeGroup\n",
      "0     726\n",
      "1    3390\n",
      "2    4488\n",
      "dtype: int64\n",
      "df_l_b_g : AgeGroup\n",
      "0    481\n",
      "1    666\n",
      "2    858\n",
      "dtype: int64\n",
      "df_l_b_c : AgeGroup\n",
      "0    514\n",
      "1    777\n",
      "2    937\n",
      "dtype: int64\n",
      "df_f_t_a : AgeGroup\n",
      "0     668\n",
      "1    2316\n",
      "2    2703\n",
      "dtype: int64\n",
      "df_f_b_g : AgeGroup\n",
      "0     831\n",
      "1    1003\n",
      "2    1070\n",
      "dtype: int64\n",
      "df_f_b_c : AgeGroup\n",
      "0    458\n",
      "2    569\n",
      "dtype: int64\n",
      "df_vf_t_a : AgeGroup\n",
      "0    207\n",
      "1    627\n",
      "2    512\n",
      "dtype: int64\n",
      "df_vf_t_g : AgeGroup\n",
      "0    209\n",
      "1    584\n",
      "2    625\n",
      "dtype: int64\n",
      "df_vf_b_g : AgeGroup\n",
      "0    113\n",
      "1    104\n",
      "2     48\n",
      "dtype: int64\n",
      "df_vf_b_c : AgeGroup\n",
      "0    119\n",
      "1    109\n",
      "2     20\n",
      "dtype: int64\n",
      "Species\n",
      "AA    8557\n",
      "AG    8604\n",
      "dtype: int64\n",
      "Species\n",
      "AC    2228\n",
      "AG    2005\n",
      "dtype: int64\n",
      "Species\n",
      "AA    5687\n",
      "dtype: int64\n",
      "Species\n",
      "AC    1027\n",
      "AG    2904\n",
      "dtype: int64\n",
      "Species\n",
      "AA    1346\n",
      "AG    1418\n",
      "dtype: int64\n",
      "Species\n",
      "AC    248\n",
      "AG    265\n",
      "dtype: int64\n",
      "Species\n",
      "AA    1106\n",
      "AC    2318\n",
      "AG    2400\n",
      "dtype: int64\n",
      "Status\n",
      "BF    1691\n",
      "GR    1391\n",
      "SF    2742\n",
      "dtype: int64\n",
      "RearCnd\n",
      "TL    5824\n",
      "dtype: int64\n",
      "AgeGroup\n",
      "0    1824\n",
      "1    2000\n",
      "2    2000\n",
      "dtype: int64\n",
      "Species\n",
      "AA    8557\n",
      "AG    8604\n",
      "dtype: int64\n",
      "Status\n",
      "BF     2818\n",
      "GR     2706\n",
      "SF    11637\n",
      "dtype: int64\n",
      "RearCnd\n",
      "TL    17161\n",
      "dtype: int64\n",
      "AgeGroup\n",
      "0    1434\n",
      "1    6864\n",
      "2    8863\n",
      "dtype: int64\n",
      "shape of X : (5824, 1625)\n",
      "shape of y age : (5824,)\n",
      "shape of y age groups : (5824,)\n",
      "shape of y species : (5824,)\n",
      "shape of y status : (5824,)\n",
      "shape of X_vf : (17161, 1625)\n",
      "shape of y_age_vf age : (17161,)\n",
      "shape of y_age_groups_vf : (17161,)\n",
      "shape of y y_species_vf : (17161,)\n",
      "shape of y y_status_vf : (17161,)\n"
     ]
    }
   ],
   "source": [
    "SelectFreqs = False\n",
    "\n",
    "df = pd.read_csv(\"/home/josh/Documents/Mosquito_Project/New_Data/Data/MIMIdata_update_19_02/mosquitoes_country_LM_5_0.dat\", '\\t')\n",
    "df.head(10)\n",
    "RearCnd_counts = df.groupby('RearCnd').size()\n",
    "print('{}'.format(RearCnd_counts))\n",
    "\n",
    "df['AgeGroup'] = 0\n",
    "df['AgeGroup'] = np.where(df['Age']>10, 2, np.where(df['Age']>4, 1, 0))\n",
    "\n",
    "df_vf = df[df['RearCnd']=='VF']\n",
    "df_vf = df_vf[df_vf['Status']=='UN']\n",
    "df = df[df['RearCnd']!='VF']\n",
    "df = df[df['Status']!='UN']\n",
    "df_l = df[df['RearCnd']=='TL']\n",
    "df_l_g = df_l[df_l['Country']=='S']\n",
    "df_l_g_a = df_l_g[df_l_g['Species']=='AA']\n",
    "age_counts = df_l_g_a.groupby('AgeGroup').size()\n",
    "print('df_l_g_a : {}'.format(age_counts))\n",
    "df_l_g_g = df_l_g[df_l_g['Species']=='AG']\n",
    "age_counts = df_l_g_g.groupby('AgeGroup').size()\n",
    "print('df_l_g_g : {}'.format(age_counts))\n",
    "df_l_g_c = df_l_g[df_l_g['Species']=='AC']\n",
    "age_counts = df_l_g_c.groupby('AgeGroup').size()\n",
    "print('df_l_g_c : {}'.format(age_counts))\n",
    "df_l_t = df_l[df_l['Country']=='T']\n",
    "df_l_t_a = df_l_t[df_l_t['Species']=='AA']\n",
    "age_counts = df_l_t_a.groupby('AgeGroup').size()\n",
    "print('df_l_t_a : {}'.format(age_counts))\n",
    "df_l_t_g = df_l_t[df_l_t['Species']=='AG']\n",
    "age_counts = df_l_t_g.groupby('AgeGroup').size()\n",
    "print('df_l_t_g : {}'.format(age_counts))\n",
    "df_l_b = df_l[df_l['Country']=='B']\n",
    "df_l_b_g = df_l_b[df_l_b['Species']=='AG']\n",
    "age_counts = df_l_b_g.groupby('AgeGroup').size()\n",
    "print('df_l_b_g : {}'.format(age_counts))\n",
    "df_l_b_c = df_l_b[df_l_b['Species']=='AC']\n",
    "age_counts = df_l_b_c.groupby('AgeGroup').size()\n",
    "print('df_l_b_c : {}'.format(age_counts))\n",
    "df_f = df[df['RearCnd']=='TF']\n",
    "df_f_t = df_f[df_f['Country']=='T']\n",
    "df_f_t_a = df_f_t[df_f_t['Species']=='AA']\n",
    "age_counts = df_f_t_a.groupby('AgeGroup').size()\n",
    "print('df_f_t_a : {}'.format(age_counts))\n",
    "# df_f_t_g = df_f_t[df_f_t['Species']=='AG'] #There isn't any\n",
    "df_f_b = df_f[df_f['Country']=='B']\n",
    "df_f_b_g = df_f_b[df_f_b['Species']=='AG']\n",
    "age_counts = df_f_b_g.groupby('AgeGroup').size()\n",
    "print('df_f_b_g : {}'.format(age_counts))\n",
    "df_f_b_c = df_f_b[df_f_b['Species']=='AC']\n",
    "age_counts = df_f_b_c.groupby('AgeGroup').size()\n",
    "print('df_f_b_c : {}'.format(age_counts))\n",
    "df_vf_t = df_vf[df_vf['Country']=='T']\n",
    "df_vf_t_a = df_vf_t[df_vf_t['Species']=='AA']\n",
    "age_counts = df_vf_t_a.groupby('AgeGroup').size()\n",
    "print('df_vf_t_a : {}'.format(age_counts))\n",
    "df_vf_t_g = df_vf_t[df_vf_t['Species']=='AG']\n",
    "age_counts = df_vf_t_g.groupby('AgeGroup').size()\n",
    "print('df_vf_t_g : {}'.format(age_counts))\n",
    "df_vf_b = df_vf[df_vf['Country']=='B']\n",
    "df_vf_b_g = df_vf_b[df_vf_b['Species']=='AG']\n",
    "age_counts = df_vf_b_g.groupby('AgeGroup').size()\n",
    "print('df_vf_b_g : {}'.format(age_counts))\n",
    "df_vf_b_c = df_vf_b[df_vf_b['Species']=='AC']\n",
    "age_counts = df_vf_b_c.groupby('AgeGroup').size()\n",
    "print('df_vf_b_c : {}'.format(age_counts))\n",
    "Species_counts = df_l_t.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_l_b.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_f_t.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_f_b.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_vf_t.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_vf_b.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "\n",
    "size_inc = 400\n",
    "for age in range(3):\n",
    "    df_temp = df_l_b_g[df_l_b_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    if age == 0:\n",
    "        df_train = df_temp.iloc[index_df_temp_inc]\n",
    "#         df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "    else:\n",
    "        df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#         df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_b_c[df_l_b_c['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 0\n",
    "for age in range(3):\n",
    "    df_temp = df_l_t_a[df_l_t_a['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    if age == 0:\n",
    "        df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "    else:\n",
    "#         df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "        df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_t_g[df_l_t_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "#     df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 400\n",
    "for age in range(3):\n",
    "    df_temp = df_l_g_a[df_l_g_a['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_g_g[df_l_g_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_g_c[df_l_g_c['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 0 # 50\n",
    "\n",
    "    \n",
    "Species_counts = df_train.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Status_counts = df_train.groupby('Status').size()\n",
    "print('{}'.format(Status_counts))\n",
    "RearCnd_counts = df_train.groupby('RearCnd').size()\n",
    "print('{}'.format(RearCnd_counts))\n",
    "age_counts = df_train.groupby('AgeGroup').size()\n",
    "print('{}'.format(age_counts))\n",
    "Species_counts = df_test.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Status_counts = df_test.groupby('Status').size()\n",
    "print('{}'.format(Status_counts))\n",
    "RearCnd_counts = df_test.groupby('RearCnd').size()\n",
    "print('{}'.format(RearCnd_counts))\n",
    "age_counts = df_test.groupby('AgeGroup').size()\n",
    "print('{}'.format(age_counts))\n",
    "\n",
    "\n",
    "if SelectFreqs:\n",
    "    X = df_train[[\"3400\",\"3276\",\"2922\",\"2854\",\"1900\",\"1746\",\"1636\",\"1540\",\"1458\",\"1306\",\"1154\",\"1076\",\"1028\",\"880\"]]\n",
    "else:\n",
    "    X = df_train.iloc[:,6:-1]\n",
    "y_age = df_train[\"Age\"]\n",
    "y_age_groups = df_train[\"AgeGroup\"]\n",
    "y_species = df_train[\"Species\"]\n",
    "y_status = df_train[\"Status\"]\n",
    "\n",
    "print('shape of X : {}'.format(X.shape))\n",
    "print('shape of y age : {}'.format(y_age.shape))\n",
    "print('shape of y age groups : {}'.format(y_age_groups.shape))\n",
    "print('shape of y species : {}'.format(y_species.shape))\n",
    "print('shape of y status : {}'.format(y_status.shape))\n",
    "\n",
    "X = np.asarray(X)\n",
    "y_age = np.asarray(y_age)\n",
    "y_age_groups = np.asarray(y_age_groups)\n",
    "y_species = np.asarray(y_species)\n",
    "y_status = np.asarray(y_status)\n",
    "\n",
    "if SelectFreqs:\n",
    "    X_vf = df_test[[\"3400\",\"3276\",\"2922\",\"2854\",\"1900\",\"1746\",\"1636\",\"1540\",\"1458\",\"1306\",\"1154\",\"1076\",\"1028\",\"880\"]]\n",
    "else:\n",
    "    X_vf = df_test.iloc[:,6:-1]\n",
    "y_age_vf = df_test[\"Age\"]\n",
    "y_age_groups_vf = df_test[\"AgeGroup\"]\n",
    "y_species_vf = df_test[\"Species\"]\n",
    "y_status_vf = df_test[\"Status\"]\n",
    "print('shape of X_vf : {}'.format(X_vf.shape))\n",
    "print('shape of y_age_vf age : {}'.format(y_age_vf.shape))\n",
    "print('shape of y_age_groups_vf : {}'.format(y_age_groups_vf.shape))\n",
    "print('shape of y y_species_vf : {}'.format(y_species_vf.shape))\n",
    "print('shape of y y_status_vf : {}'.format(y_status_vf.shape))\n",
    "X_vf = np.asarray(X_vf)\n",
    "y_age_vf = np.asarray(y_age_vf)\n",
    "y_age_groups_vf = np.asarray(y_age_groups_vf)\n",
    "y_species_vf = np.asarray(y_species_vf)\n",
    "y_status_vf = np.asarray(y_status_vf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Running\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 1625, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv1D)                  (None, 1618, 16)     144         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_1 (BatchNormalization (None, 1618, 16)     64          Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1618, 16)     0           batchnorm_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv2 (Conv1D)                  (None, 806, 16)      2064        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_2 (BatchNormalization (None, 806, 16)      64          Conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 806, 16)      0           batchnorm_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv3 (Conv1D)                  (None, 804, 16)      784         max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_3 (BatchNormalization (None, 804, 16)      64          Conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 804, 16)      0           batchnorm_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv4 (Conv1D)                  (None, 400, 16)      1552        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_4 (BatchNormalization (None, 400, 16)      64          Conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 400, 16)      0           batchnorm_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv5 (Conv1D)                  (None, 396, 16)      1296        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_5 (BatchNormalization (None, 396, 16)      64          Conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 198, 16)      0           batchnorm_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3168)         0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dout6 (Dropout)                 (None, 3168)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "d6 (Dense)                      (None, 500)          1584500     dout6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_6 (BatchNormalization (None, 500)          2000        d6[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "age_group (Dense)               (None, 3)            1503        batchnorm_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "species (Dense)                 (None, 3)            1503        batchnorm_6[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,595,666\n",
      "Trainable params: 1,594,506\n",
      "Non-trainable params: 1,160\n",
      "__________________________________________________________________________________________________\n",
      "583/583 [==============================] - 0s 122us/step\n",
      "Fold 1 Running\n",
      "583/583 [==============================] - 0s 363us/step\n",
      "Fold 2 Running\n",
      "583/583 [==============================] - 0s 89us/step\n",
      "Fold 3 Running\n",
      "583/583 [==============================] - 0s 193us/step\n",
      "Fold 4 Running\n",
      "582/582 [==============================] - 0s 169us/step\n",
      "Fold 5 Running\n",
      "582/582 [==============================] - 0s 162us/step\n",
      "Fold 6 Running\n",
      "582/582 [==============================] - 0s 85us/step\n",
      "Fold 7 Running\n",
      "582/582 [==============================] - 0s 174us/step\n",
      "Fold 8 Running\n",
      "582/582 [==============================] - 0s 174us/step\n",
      "Fold 9 Running\n",
      "582/582 [==============================] - 0s 178us/step\n",
      "Run time : 33645.6858420372 s\n",
      "Run time : 560.76143070062 m\n",
      "Run time : 9.346023845010334 h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/.local/lib/python3.6/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "input_layer_dim = len(X[0])\n",
    "\n",
    "## Train\n",
    "y_age_groups_list = [[age] for age in y_age_groups]\n",
    "y_species_list = [[species] for species in y_species]\n",
    "\n",
    "## Test\n",
    "y_age_groups_list_test = [[age] for age in y_age_groups_vf]\n",
    "y_species_list_test = [[species] for species in y_species_vf]\n",
    "\n",
    "age_mlb = MultiLabelBinarizer().fit(np.vstack((np.array(y_age_groups_list),np.array(y_age_groups_list_test))))\n",
    "age_groups = age_mlb.transform(np.array(y_age_groups_list))\n",
    "age_groups_test = age_mlb.transform(np.array(y_age_groups_list_test))\n",
    "\n",
    "species_mlb = MultiLabelBinarizer().fit(np.vstack((np.array(y_species_list),np.array(y_species_list_test))))\n",
    "species = species_mlb.transform(np.array(y_species_list))\n",
    "species_test = species_mlb.transform(np.array(y_species_list_test))\n",
    "\n",
    "age_group_classes = [\"1-4\", \"5-10\", \"11-17\"]\n",
    "species_classes = list(np.unique(np.vstack((np.array(y_species_list),np.array(y_species_list_test)))))\n",
    "\n",
    "## Labels default - all classification\n",
    "label, classes_default, outputs_default = [age_groups, species], [age_group_classes, species_classes], ['xAgeGroup', 'xSpecies']\n",
    "## Labels default - all classification\n",
    "label_test = [age_groups_test, species_test]\n",
    "\n",
    "## Set Up\n",
    "outdir = \"Results_Paper/\"\n",
    "build_folder(outdir, False)\n",
    "SelectFreqs = False\n",
    "\n",
    "## Declare and train the model\n",
    "model_size = [{'type':'c', 'filter':16, 'kernel':8, 'stride':1, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':8, 'stride':2, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':3, 'stride':1, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':6, 'stride':2, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':5, 'stride':1, 'pooling':2},\n",
    "             {'type':'d', 'width':500}]\n",
    "\n",
    "## Name the model\n",
    "model_name = 'Baseline_CNN'    \n",
    "    \n",
    "histories = []\n",
    "fold = 1\n",
    "train_model = True\n",
    "\n",
    "## Name a folder for the outputs to go into\n",
    "savedir = (outdir+\"Trian_Lab_Only_No_Tanzania/\")\n",
    "build_folder(savedir, True)\n",
    "    \n",
    "start_time = time()\n",
    "save_predicted = []\n",
    "save_true = []\n",
    "\n",
    "## Scale train, test\n",
    "scl = StandardScaler()\n",
    "features_scl = scl.fit(X=np.vstack((X, X_vf)))\n",
    "features = features_scl.transform(X=X)\n",
    "features_test = features_scl.transform(X=X_vf)\n",
    "\n",
    "## Split into training / testing\n",
    "# test_splits = train_test_split(features, *(label), test_size=0.1, shuffle=True, random_state=rand_seed)\n",
    "## Pack up data\n",
    "# X_train = test_splits.pop(0)\n",
    "# X_test = test_splits.pop(0)\n",
    "# y_train = test_splits[::2]\n",
    "# y_test = test_splits[1::2]\n",
    "X_train = features\n",
    "X_test = features_test\n",
    "y_train = label\n",
    "y_test = label_test\n",
    "\n",
    "if not SelectFreqs:\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "\n",
    "model_to_test = {\n",
    "    \"model_shape\" : [model_size], # defines the hidden layers of the model\n",
    "    \"model_name\"  : [model_name],\n",
    "    \"input_layer_dim\"  : [input_layer_dim], # size of input layer\n",
    "    \"model_ver_num\"  : [0],\n",
    "    \"fold\"  : [fold], # kf.split number on\n",
    "    \"labels\"   : [y_train],\n",
    "    \"features\" : [X_train],\n",
    "    \"classes\"  : [classes_default],\n",
    "    \"outputs\"   : [outputs_default],\n",
    "    \"compile_loss\": [{'age': 'categorical_crossentropy'}],\n",
    "    \"compile_metrics\" :[{'age': 'accuracy'}]\n",
    "}\n",
    "\n",
    "## Call function to train all the models from the dictionary\n",
    "model, history = train_models(model_to_test, savedir, SelectFreqs=SelectFreqs)\n",
    "histories.append(history)\n",
    "\n",
    "predicted_labels = list([] for i in range(len(y_train)))\n",
    "true_labels = list([] for i in range(len(y_train)))\n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "predicted_labels = [x+[y] for x,y in zip(predicted_labels,y_predicted)]\n",
    "true_labels = [x+[y] for x,y in zip(true_labels,y_test)]\n",
    "\n",
    "predicted_labels = [predicted_labels[i][0].tolist() for i in range(len(predicted_labels))]\n",
    "true_labels = [true_labels[i][0].tolist() for i in range(len(true_labels))]\n",
    "\n",
    "for pred, tru in zip(predicted_labels, true_labels):\n",
    "    save_predicted.append(pred)\n",
    "    save_true.append(tru)\n",
    "\n",
    "## Visualize the results\n",
    "visualize(histories, savedir, model_name, str(fold), classes_default, outputs_default, predicted_labels, true_labels)\n",
    "# log_data(test_index, 'test_index', fold, savedir)\n",
    "\n",
    "# Clear the Keras session, otherwise it will keep adding new\n",
    "# models to the same TensorFlow graph each time we create\n",
    "# a model with a different set of hyper-parameters.\n",
    "K.clear_session()\n",
    "\n",
    "# Delete the Keras model with these hyper-parameters from memory.\n",
    "del model\n",
    "\n",
    "# visualize(1, savedir, model_name, \"Averaged\", classes_default, outputs_default, save_predicted, save_true)\n",
    "end_time = time()\n",
    "print('Run time : {} s'.format(end_time-start_time))\n",
    "print('Run time : {} m'.format((end_time-start_time)/60))\n",
    "print('Run time : {} h'.format((end_time-start_time)/3600))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/.local/lib/python3.6/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "input_layer_dim = len(X[0])\n",
    "\n",
    "## Train\n",
    "y_age_groups_list = [[age] for age in y_age_groups]\n",
    "y_species_list = [[species] for species in y_species]\n",
    "\n",
    "## Test\n",
    "y_age_groups_list_test = [[age] for age in y_age_groups_vf]\n",
    "y_species_list_test = [[species] for species in y_species_vf]\n",
    "\n",
    "age_mlb = MultiLabelBinarizer().fit(np.vstack((np.array(y_age_groups_list),np.array(y_age_groups_list_test))))\n",
    "age_groups = age_mlb.transform(np.array(y_age_groups_list))\n",
    "age_groups_test = age_mlb.transform(np.array(y_age_groups_list_test))\n",
    "\n",
    "species_mlb = MultiLabelBinarizer().fit(np.vstack((np.array(y_species_list),np.array(y_species_list_test))))\n",
    "species = species_mlb.transform(np.array(y_species_list))\n",
    "species_test = species_mlb.transform(np.array(y_species_list_test))\n",
    "\n",
    "age_group_classes = [\"1-4\", \"5-10\", \"11-17\"]\n",
    "species_classes = list(np.unique(np.vstack((np.array(y_species_list),np.array(y_species_list_test)))))\n",
    "\n",
    "## Labels default - all classification\n",
    "label, classes_default, outputs_default = [age_groups, species], [age_group_classes, species_classes], ['xAgeGroup', 'xSpecies']\n",
    "## Labels default - all classification\n",
    "label_test = [age_groups_test, species_test]\n",
    "\n",
    "## Set Up\n",
    "outdir = \"Results_Paper/\"\n",
    "build_folder(outdir, False)\n",
    "SelectFreqs = False\n",
    "\n",
    "## Name the model\n",
    "model_name = 'Baseline_CNN'    \n",
    "    \n",
    "histories = []\n",
    "fold = 1\n",
    "train_model = False\n",
    "\n",
    "## Name a folder for the outputs to go into\n",
    "savedir = (outdir+\"Trian_Lab_Only_No_Tanzania/\")\n",
    "build_folder(savedir, False)\n",
    "    \n",
    "start_time = time()\n",
    "save_predicted = []\n",
    "save_true = []\n",
    "\n",
    "## Scale train, test\n",
    "scl = StandardScaler()\n",
    "features_scl = scl.fit(X=np.vstack((X, X_vf)))\n",
    "features = features_scl.transform(X=X)\n",
    "features_test = features_scl.transform(X=X_vf)\n",
    "\n",
    "X_train = features\n",
    "X_test = features_test\n",
    "y_train = label\n",
    "y_test = label_test\n",
    "\n",
    "if not SelectFreqs:\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "model = load_model((savedir+\"Best_Model.h5\"))\n",
    "print('Model loaded successfully') \n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "predicted_labels = list([] for i in range(len(y_train)))\n",
    "true_labels = list([] for i in range(len(y_train)))\n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "predicted_labels = [x+[y] for x,y in zip(predicted_labels,y_predicted)]\n",
    "true_labels = [x+[y] for x,y in zip(true_labels,y_test)]\n",
    "\n",
    "predicted_labels = [predicted_labels[i][0].tolist() for i in range(len(predicted_labels))]\n",
    "true_labels = [true_labels[i][0].tolist() for i in range(len(true_labels))]\n",
    "\n",
    "for pred, tru in zip(predicted_labels, true_labels):\n",
    "    save_predicted.append(pred)\n",
    "    save_true.append(tru)\n",
    "\n",
    "## Visualize the results\n",
    "visualize(histories, savedir, model_name, str(fold), classes_default, outputs_default, predicted_labels, true_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Glasgow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RearCnd\n",
      "TF     9618\n",
      "TL    27838\n",
      "VF     3285\n",
      "dtype: int64\n",
      "df_l_g_a : AgeGroup\n",
      "0    306\n",
      "1    592\n",
      "2    763\n",
      "dtype: int64\n",
      "df_l_g_g : AgeGroup\n",
      "0    629\n",
      "1    827\n",
      "2    821\n",
      "dtype: int64\n",
      "df_l_g_c : AgeGroup\n",
      "0     318\n",
      "1     892\n",
      "2    1282\n",
      "dtype: int64\n",
      "df_l_t_a : AgeGroup\n",
      "0     708\n",
      "1    3474\n",
      "2    4375\n",
      "dtype: int64\n",
      "df_l_t_g : AgeGroup\n",
      "0     726\n",
      "1    3390\n",
      "2    4488\n",
      "dtype: int64\n",
      "df_l_b_g : AgeGroup\n",
      "0    481\n",
      "1    666\n",
      "2    858\n",
      "dtype: int64\n",
      "df_l_b_c : AgeGroup\n",
      "0    514\n",
      "1    777\n",
      "2    937\n",
      "dtype: int64\n",
      "df_f_t_a : AgeGroup\n",
      "0     668\n",
      "1    2316\n",
      "2    2703\n",
      "dtype: int64\n",
      "df_f_b_g : AgeGroup\n",
      "0     831\n",
      "1    1003\n",
      "2    1070\n",
      "dtype: int64\n",
      "df_f_b_c : AgeGroup\n",
      "0    458\n",
      "2    569\n",
      "dtype: int64\n",
      "df_vf_t_a : AgeGroup\n",
      "0    207\n",
      "1    627\n",
      "2    512\n",
      "dtype: int64\n",
      "df_vf_t_g : AgeGroup\n",
      "0    209\n",
      "1    584\n",
      "2    625\n",
      "dtype: int64\n",
      "df_vf_b_g : AgeGroup\n",
      "0    113\n",
      "1    104\n",
      "2     48\n",
      "dtype: int64\n",
      "df_vf_b_c : AgeGroup\n",
      "0    119\n",
      "1    109\n",
      "2     20\n",
      "dtype: int64\n",
      "Species\n",
      "AA    8557\n",
      "AG    8604\n",
      "dtype: int64\n",
      "Species\n",
      "AC    2228\n",
      "AG    2005\n",
      "dtype: int64\n",
      "Species\n",
      "AA    5687\n",
      "dtype: int64\n",
      "Species\n",
      "AC    1027\n",
      "AG    2904\n",
      "dtype: int64\n",
      "Species\n",
      "AA    1346\n",
      "AG    1418\n",
      "dtype: int64\n",
      "Species\n",
      "AC    248\n",
      "AG    265\n",
      "dtype: int64\n",
      "Species\n",
      "AA    1200\n",
      "AC    1200\n",
      "AG    2400\n",
      "dtype: int64\n",
      "Status\n",
      "BF    1167\n",
      "GR     832\n",
      "SF    2801\n",
      "dtype: int64\n",
      "RearCnd\n",
      "TL    4800\n",
      "dtype: int64\n",
      "AgeGroup\n",
      "0    1600\n",
      "1    1600\n",
      "2    1600\n",
      "dtype: int64\n",
      "Species\n",
      "AA    1661\n",
      "AC    2492\n",
      "AG    2277\n",
      "dtype: int64\n",
      "Status\n",
      "BF    1807\n",
      "GR    1664\n",
      "SF    2959\n",
      "dtype: int64\n",
      "RearCnd\n",
      "TL    6430\n",
      "dtype: int64\n",
      "AgeGroup\n",
      "0    1253\n",
      "1    2311\n",
      "2    2866\n",
      "dtype: int64\n",
      "shape of X : (4800, 1625)\n",
      "shape of y age : (4800,)\n",
      "shape of y age groups : (4800,)\n",
      "shape of y species : (4800,)\n",
      "shape of y status : (4800,)\n",
      "shape of X_vf : (6430, 1625)\n",
      "shape of y_age_vf age : (6430,)\n",
      "shape of y_age_groups_vf : (6430,)\n",
      "shape of y y_species_vf : (6430,)\n",
      "shape of y y_status_vf : (6430,)\n"
     ]
    }
   ],
   "source": [
    "SelectFreqs = False\n",
    "\n",
    "df = pd.read_csv(\"/home/josh/Documents/Mosquito_Project/New_Data/Data/MIMIdata_update_19_02/mosquitoes_country_LM_5_0.dat\", '\\t')\n",
    "df.head(10)\n",
    "RearCnd_counts = df.groupby('RearCnd').size()\n",
    "print('{}'.format(RearCnd_counts))\n",
    "\n",
    "df['AgeGroup'] = 0\n",
    "df['AgeGroup'] = np.where(df['Age']>10, 2, np.where(df['Age']>4, 1, 0))\n",
    "\n",
    "df_vf = df[df['RearCnd']=='VF']\n",
    "df_vf = df_vf[df_vf['Status']=='UN']\n",
    "df = df[df['RearCnd']!='VF']\n",
    "df = df[df['Status']!='UN']\n",
    "df_l = df[df['RearCnd']=='TL']\n",
    "df_l_g = df_l[df_l['Country']=='S']\n",
    "df_l_g_a = df_l_g[df_l_g['Species']=='AA']\n",
    "age_counts = df_l_g_a.groupby('AgeGroup').size()\n",
    "print('df_l_g_a : {}'.format(age_counts))\n",
    "df_l_g_g = df_l_g[df_l_g['Species']=='AG']\n",
    "age_counts = df_l_g_g.groupby('AgeGroup').size()\n",
    "print('df_l_g_g : {}'.format(age_counts))\n",
    "df_l_g_c = df_l_g[df_l_g['Species']=='AC']\n",
    "age_counts = df_l_g_c.groupby('AgeGroup').size()\n",
    "print('df_l_g_c : {}'.format(age_counts))\n",
    "df_l_t = df_l[df_l['Country']=='T']\n",
    "df_l_t_a = df_l_t[df_l_t['Species']=='AA']\n",
    "age_counts = df_l_t_a.groupby('AgeGroup').size()\n",
    "print('df_l_t_a : {}'.format(age_counts))\n",
    "df_l_t_g = df_l_t[df_l_t['Species']=='AG']\n",
    "age_counts = df_l_t_g.groupby('AgeGroup').size()\n",
    "print('df_l_t_g : {}'.format(age_counts))\n",
    "df_l_b = df_l[df_l['Country']=='B']\n",
    "df_l_b_g = df_l_b[df_l_b['Species']=='AG']\n",
    "age_counts = df_l_b_g.groupby('AgeGroup').size()\n",
    "print('df_l_b_g : {}'.format(age_counts))\n",
    "df_l_b_c = df_l_b[df_l_b['Species']=='AC']\n",
    "age_counts = df_l_b_c.groupby('AgeGroup').size()\n",
    "print('df_l_b_c : {}'.format(age_counts))\n",
    "df_f = df[df['RearCnd']=='TF']\n",
    "df_f_t = df_f[df_f['Country']=='T']\n",
    "df_f_t_a = df_f_t[df_f_t['Species']=='AA']\n",
    "age_counts = df_f_t_a.groupby('AgeGroup').size()\n",
    "print('df_f_t_a : {}'.format(age_counts))\n",
    "# df_f_t_g = df_f_t[df_f_t['Species']=='AG'] #There isn't any\n",
    "df_f_b = df_f[df_f['Country']=='B']\n",
    "df_f_b_g = df_f_b[df_f_b['Species']=='AG']\n",
    "age_counts = df_f_b_g.groupby('AgeGroup').size()\n",
    "print('df_f_b_g : {}'.format(age_counts))\n",
    "df_f_b_c = df_f_b[df_f_b['Species']=='AC']\n",
    "age_counts = df_f_b_c.groupby('AgeGroup').size()\n",
    "print('df_f_b_c : {}'.format(age_counts))\n",
    "df_vf_t = df_vf[df_vf['Country']=='T']\n",
    "df_vf_t_a = df_vf_t[df_vf_t['Species']=='AA']\n",
    "age_counts = df_vf_t_a.groupby('AgeGroup').size()\n",
    "print('df_vf_t_a : {}'.format(age_counts))\n",
    "df_vf_t_g = df_vf_t[df_vf_t['Species']=='AG']\n",
    "age_counts = df_vf_t_g.groupby('AgeGroup').size()\n",
    "print('df_vf_t_g : {}'.format(age_counts))\n",
    "df_vf_b = df_vf[df_vf['Country']=='B']\n",
    "df_vf_b_g = df_vf_b[df_vf_b['Species']=='AG']\n",
    "age_counts = df_vf_b_g.groupby('AgeGroup').size()\n",
    "print('df_vf_b_g : {}'.format(age_counts))\n",
    "df_vf_b_c = df_vf_b[df_vf_b['Species']=='AC']\n",
    "age_counts = df_vf_b_c.groupby('AgeGroup').size()\n",
    "print('df_vf_b_c : {}'.format(age_counts))\n",
    "Species_counts = df_l_t.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_l_b.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_f_t.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_f_b.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_vf_t.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_vf_b.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "\n",
    "size_inc = 400\n",
    "for age in range(3):\n",
    "    df_temp = df_l_b_g[df_l_b_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    if age == 0:\n",
    "        df_train = df_temp.iloc[index_df_temp_inc]\n",
    "#         df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "    else:\n",
    "        df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#         df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_b_c[df_l_b_c['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 400\n",
    "for age in range(3):\n",
    "    df_temp = df_l_t_a[df_l_t_a['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_t_g[df_l_t_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 0\n",
    "for age in range(3):\n",
    "    df_temp = df_l_g_a[df_l_g_a['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    if age == 0:\n",
    "        df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "    else:\n",
    "#         df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "        df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_g_g[df_l_g_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "#     df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_g_c[df_l_g_c['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "#     df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "\n",
    "    \n",
    "Species_counts = df_train.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Status_counts = df_train.groupby('Status').size()\n",
    "print('{}'.format(Status_counts))\n",
    "RearCnd_counts = df_train.groupby('RearCnd').size()\n",
    "print('{}'.format(RearCnd_counts))\n",
    "age_counts = df_train.groupby('AgeGroup').size()\n",
    "print('{}'.format(age_counts))\n",
    "Species_counts = df_test.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Status_counts = df_test.groupby('Status').size()\n",
    "print('{}'.format(Status_counts))\n",
    "RearCnd_counts = df_test.groupby('RearCnd').size()\n",
    "print('{}'.format(RearCnd_counts))\n",
    "age_counts = df_test.groupby('AgeGroup').size()\n",
    "print('{}'.format(age_counts))\n",
    "\n",
    "\n",
    "if SelectFreqs:\n",
    "    X = df_train[[\"3400\",\"3276\",\"2922\",\"2854\",\"1900\",\"1746\",\"1636\",\"1540\",\"1458\",\"1306\",\"1154\",\"1076\",\"1028\",\"880\"]]\n",
    "else:\n",
    "    X = df_train.iloc[:,6:-1]\n",
    "y_age = df_train[\"Age\"]\n",
    "y_age_groups = df_train[\"AgeGroup\"]\n",
    "y_species = df_train[\"Species\"]\n",
    "y_status = df_train[\"Status\"]\n",
    "\n",
    "print('shape of X : {}'.format(X.shape))\n",
    "print('shape of y age : {}'.format(y_age.shape))\n",
    "print('shape of y age groups : {}'.format(y_age_groups.shape))\n",
    "print('shape of y species : {}'.format(y_species.shape))\n",
    "print('shape of y status : {}'.format(y_status.shape))\n",
    "\n",
    "X = np.asarray(X)\n",
    "y_age = np.asarray(y_age)\n",
    "y_age_groups = np.asarray(y_age_groups)\n",
    "y_species = np.asarray(y_species)\n",
    "y_status = np.asarray(y_status)\n",
    "\n",
    "if SelectFreqs:\n",
    "    X_vf = df_test[[\"3400\",\"3276\",\"2922\",\"2854\",\"1900\",\"1746\",\"1636\",\"1540\",\"1458\",\"1306\",\"1154\",\"1076\",\"1028\",\"880\"]]\n",
    "else:\n",
    "    X_vf = df_test.iloc[:,6:-1]\n",
    "y_age_vf = df_test[\"Age\"]\n",
    "y_age_groups_vf = df_test[\"AgeGroup\"]\n",
    "y_species_vf = df_test[\"Species\"]\n",
    "y_status_vf = df_test[\"Status\"]\n",
    "print('shape of X_vf : {}'.format(X_vf.shape))\n",
    "print('shape of y_age_vf age : {}'.format(y_age_vf.shape))\n",
    "print('shape of y_age_groups_vf : {}'.format(y_age_groups_vf.shape))\n",
    "print('shape of y y_species_vf : {}'.format(y_species_vf.shape))\n",
    "print('shape of y y_status_vf : {}'.format(y_status_vf.shape))\n",
    "X_vf = np.asarray(X_vf)\n",
    "y_age_vf = np.asarray(y_age_vf)\n",
    "y_age_groups_vf = np.asarray(y_age_groups_vf)\n",
    "y_species_vf = np.asarray(y_species_vf)\n",
    "y_status_vf = np.asarray(y_status_vf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Running\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 1625, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv1D)                  (None, 1618, 16)     144         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_1 (BatchNormalization (None, 1618, 16)     64          Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1618, 16)     0           batchnorm_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv2 (Conv1D)                  (None, 806, 16)      2064        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_2 (BatchNormalization (None, 806, 16)      64          Conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 806, 16)      0           batchnorm_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv3 (Conv1D)                  (None, 804, 16)      784         max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_3 (BatchNormalization (None, 804, 16)      64          Conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 804, 16)      0           batchnorm_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv4 (Conv1D)                  (None, 400, 16)      1552        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_4 (BatchNormalization (None, 400, 16)      64          Conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 400, 16)      0           batchnorm_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv5 (Conv1D)                  (None, 396, 16)      1296        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_5 (BatchNormalization (None, 396, 16)      64          Conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 198, 16)      0           batchnorm_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3168)         0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dout6 (Dropout)                 (None, 3168)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "d6 (Dense)                      (None, 500)          1584500     dout6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_6 (BatchNormalization (None, 500)          2000        d6[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "age_group (Dense)               (None, 3)            1503        batchnorm_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "species (Dense)                 (None, 3)            1503        batchnorm_6[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,595,666\n",
      "Trainable params: 1,594,506\n",
      "Non-trainable params: 1,160\n",
      "__________________________________________________________________________________________________\n",
      "480/480 [==============================] - 0s 153us/step\n",
      "Fold 1 Running\n",
      "480/480 [==============================] - 0s 152us/step\n",
      "Fold 2 Running\n",
      "480/480 [==============================] - 0s 174us/step\n",
      "Fold 3 Running\n",
      "480/480 [==============================] - 0s 71us/step\n",
      "Fold 4 Running\n",
      "480/480 [==============================] - 0s 172us/step\n",
      "Fold 5 Running\n",
      "480/480 [==============================] - 0s 155us/step\n",
      "Fold 6 Running\n",
      "480/480 [==============================] - 0s 87us/step\n",
      "Fold 7 Running\n",
      "480/480 [==============================] - 0s 178us/step\n",
      "Fold 8 Running\n",
      "480/480 [==============================] - 0s 186us/step\n",
      "Fold 9 Running\n",
      "480/480 [==============================] - 0s 183us/step\n",
      "Run time : 26140.025050640106 s\n",
      "Run time : 435.6670841773351 m\n",
      "Run time : 7.261118069622252 h\n"
     ]
    }
   ],
   "source": [
    "input_layer_dim = len(X[0])\n",
    "\n",
    "## Train\n",
    "y_age_groups_list = [[age] for age in y_age_groups]\n",
    "y_species_list = [[species] for species in y_species]\n",
    "\n",
    "## Test\n",
    "y_age_groups_list_test = [[age] for age in y_age_groups_vf]\n",
    "y_species_list_test = [[species] for species in y_species_vf]\n",
    "\n",
    "age_mlb = MultiLabelBinarizer().fit(np.vstack((np.array(y_age_groups_list),np.array(y_age_groups_list_test))))\n",
    "age_groups = age_mlb.transform(np.array(y_age_groups_list))\n",
    "age_groups_test = age_mlb.transform(np.array(y_age_groups_list_test))\n",
    "\n",
    "species_mlb = MultiLabelBinarizer().fit(np.vstack((np.array(y_species_list),np.array(y_species_list_test))))\n",
    "species = species_mlb.transform(np.array(y_species_list))\n",
    "species_test = species_mlb.transform(np.array(y_species_list_test))\n",
    "\n",
    "age_group_classes = [\"1-4\", \"5-10\", \"11-17\"]\n",
    "species_classes = list(np.unique(np.vstack((np.array(y_species_list),np.array(y_species_list_test)))))\n",
    "\n",
    "## Labels default - all classification\n",
    "label, classes_default, outputs_default = [age_groups, species], [age_group_classes, species_classes], ['xAgeGroup', 'xSpecies']\n",
    "## Labels default - all classification\n",
    "label_test = [age_groups_test, species_test]\n",
    "\n",
    "\n",
    "outdir = \"Results_Paper/\"\n",
    "build_folder(outdir, False)\n",
    "SelectFreqs = False\n",
    "\n",
    "\n",
    "## Declare and train the model\n",
    "model_size = [{'type':'c', 'filter':16, 'kernel':8, 'stride':1, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':8, 'stride':2, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':3, 'stride':1, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':6, 'stride':2, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':5, 'stride':1, 'pooling':2},\n",
    "             {'type':'d', 'width':500}]\n",
    "\n",
    "## Name the model\n",
    "model_name = 'Baseline_CNN'    \n",
    "    \n",
    "histories = []\n",
    "fold = 1\n",
    "train_model = True\n",
    "\n",
    "## Name a folder for the outputs to go into\n",
    "savedir = (outdir+\"Trian_Lab_Only_No_Glasgow/\")\n",
    "build_folder(savedir, True)\n",
    "    \n",
    "start_time = time()\n",
    "save_predicted = []\n",
    "save_true = []\n",
    "\n",
    "## Scale train, test\n",
    "scl = StandardScaler()\n",
    "features_scl = scl.fit(X=np.vstack((X, X_vf)))\n",
    "features = features_scl.transform(X=X)\n",
    "features_test = features_scl.transform(X=X_vf)\n",
    "\n",
    "## Split into training / testing\n",
    "# test_splits = train_test_split(features, *(label), test_size=0.1, shuffle=True, random_state=rand_seed)\n",
    "## Pack up data\n",
    "# X_train = test_splits.pop(0)\n",
    "# X_test = test_splits.pop(0)\n",
    "# y_train = test_splits[::2]\n",
    "# y_test = test_splits[1::2]\n",
    "X_train = features\n",
    "X_test = features_test\n",
    "y_train = label\n",
    "y_test = label_test\n",
    "\n",
    "\n",
    "if not SelectFreqs:\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "\n",
    "model_to_test = {\n",
    "    \"model_shape\" : [model_size], # defines the hidden layers of the model\n",
    "    \"model_name\"  : [model_name],\n",
    "    \"input_layer_dim\"  : [input_layer_dim], # size of input layer\n",
    "    \"model_ver_num\"  : [0],\n",
    "    \"fold\"  : [fold], # kf.split number on\n",
    "    \"labels\"   : [y_train],\n",
    "    \"features\" : [X_train],\n",
    "    \"classes\"  : [classes_default],\n",
    "    \"outputs\"   : [outputs_default],\n",
    "    \"compile_loss\": [{'age': 'categorical_crossentropy'}],\n",
    "    \"compile_metrics\" :[{'age': 'accuracy'}]\n",
    "}\n",
    "\n",
    "## Call function to train all the models from the dictionary\n",
    "model, history = train_models(model_to_test, savedir, SelectFreqs=SelectFreqs)\n",
    "histories.append(history)\n",
    "\n",
    "predicted_labels = list([] for i in range(len(y_train)))\n",
    "true_labels = list([] for i in range(len(y_train)))\n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "predicted_labels = [x+[y] for x,y in zip(predicted_labels,y_predicted)]\n",
    "true_labels = [x+[y] for x,y in zip(true_labels,y_test)]\n",
    "\n",
    "predicted_labels = [predicted_labels[i][0].tolist() for i in range(len(predicted_labels))]\n",
    "true_labels = [true_labels[i][0].tolist() for i in range(len(true_labels))]\n",
    "\n",
    "for pred, tru in zip(predicted_labels, true_labels):\n",
    "    save_predicted.append(pred)\n",
    "    save_true.append(tru)\n",
    "\n",
    "## Visualize the results\n",
    "visualize(histories, savedir, model_name, str(fold), classes_default, outputs_default, predicted_labels, true_labels)\n",
    "# log_data(test_index, 'test_index', fold, savedir)\n",
    "\n",
    "# Clear the Keras session, otherwise it will keep adding new\n",
    "# models to the same TensorFlow graph each time we create\n",
    "# a model with a different set of hyper-parameters.\n",
    "K.clear_session()\n",
    "\n",
    "# Delete the Keras model with these hyper-parameters from memory.\n",
    "del model\n",
    "\n",
    "# visualize(1, savedir, model_name, \"Averaged\", classes_default, outputs_default, save_predicted, save_true)\n",
    "end_time = time()\n",
    "print('Run time : {} s'.format(end_time-start_time))\n",
    "print('Run time : {} m'.format((end_time-start_time)/60))\n",
    "print('Run time : {} h'.format((end_time-start_time)/3600))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "input_layer_dim = len(X[0])\n",
    "\n",
    "## Train\n",
    "y_age_groups_list = [[age] for age in y_age_groups]\n",
    "y_species_list = [[species] for species in y_species]\n",
    "\n",
    "## Test\n",
    "y_age_groups_list_test = [[age] for age in y_age_groups_vf]\n",
    "y_species_list_test = [[species] for species in y_species_vf]\n",
    "\n",
    "age_mlb = MultiLabelBinarizer().fit(np.vstack((np.array(y_age_groups_list),np.array(y_age_groups_list_test))))\n",
    "age_groups = age_mlb.transform(np.array(y_age_groups_list))\n",
    "age_groups_test = age_mlb.transform(np.array(y_age_groups_list_test))\n",
    "\n",
    "species_mlb = MultiLabelBinarizer().fit(np.vstack((np.array(y_species_list),np.array(y_species_list_test))))\n",
    "species = species_mlb.transform(np.array(y_species_list))\n",
    "species_test = species_mlb.transform(np.array(y_species_list_test))\n",
    "\n",
    "age_group_classes = [\"1-4\", \"5-10\", \"11-17\"]\n",
    "species_classes = list(np.unique(np.vstack((np.array(y_species_list),np.array(y_species_list_test)))))\n",
    "\n",
    "## Labels default - all classification\n",
    "label, classes_default, outputs_default = [age_groups, species], [age_group_classes, species_classes], ['xAgeGroup', 'xSpecies']\n",
    "## Labels default - all classification\n",
    "label_test = [age_groups_test, species_test]\n",
    "\n",
    "\n",
    "outdir = \"Results_Paper/\"\n",
    "build_folder(outdir, False)\n",
    "SelectFreqs = False\n",
    "\n",
    "## Name the model\n",
    "model_name = 'Baseline_CNN'    \n",
    "    \n",
    "histories = []\n",
    "fold = 1\n",
    "train_model = False\n",
    "\n",
    "## Name a folder for the outputs to go into\n",
    "savedir = (outdir+\"Trian_Lab_Only_No_Glasgow/\")\n",
    "build_folder(savedir, False)\n",
    "    \n",
    "start_time = time()\n",
    "save_predicted = []\n",
    "save_true = []\n",
    "\n",
    "## Scale train, test\n",
    "scl = StandardScaler()\n",
    "features_scl = scl.fit(X=np.vstack((X, X_vf)))\n",
    "features = features_scl.transform(X=X)\n",
    "features_test = features_scl.transform(X=X_vf)\n",
    "\n",
    "## Split into training / testing\n",
    "# test_splits = train_test_split(features, *(label), test_size=0.1, shuffle=True, random_state=rand_seed)\n",
    "## Pack up data\n",
    "# X_train = test_splits.pop(0)\n",
    "# X_test = test_splits.pop(0)\n",
    "# y_train = test_splits[::2]\n",
    "# y_test = test_splits[1::2]\n",
    "X_train = features\n",
    "X_test = features_test\n",
    "y_train = label\n",
    "y_test = label_test\n",
    "\n",
    "\n",
    "if not SelectFreqs:\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "model = load_model((savedir+\"Best_Model.h5\"))\n",
    "print('Model loaded successfully') \n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "predicted_labels = list([] for i in range(len(y_train)))\n",
    "true_labels = list([] for i in range(len(y_train)))\n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "predicted_labels = [x+[y] for x,y in zip(predicted_labels,y_predicted)]\n",
    "true_labels = [x+[y] for x,y in zip(true_labels,y_test)]\n",
    "\n",
    "predicted_labels = [predicted_labels[i][0].tolist() for i in range(len(predicted_labels))]\n",
    "true_labels = [true_labels[i][0].tolist() for i in range(len(true_labels))]\n",
    "\n",
    "for pred, tru in zip(predicted_labels, true_labels):\n",
    "    save_predicted.append(pred)\n",
    "    save_true.append(tru)\n",
    "\n",
    "## Visualize the results\n",
    "visualize(histories, savedir, model_name, str(fold), classes_default, outputs_default, predicted_labels, true_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training All Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RearCnd\n",
      "TF     9618\n",
      "TL    27838\n",
      "VF     3285\n",
      "dtype: int64\n",
      "df_l_g_a : AgeGroup\n",
      "0    306\n",
      "1    592\n",
      "2    763\n",
      "dtype: int64\n",
      "df_l_g_g : AgeGroup\n",
      "0    629\n",
      "1    827\n",
      "2    821\n",
      "dtype: int64\n",
      "df_l_g_c : AgeGroup\n",
      "0     318\n",
      "1     892\n",
      "2    1282\n",
      "dtype: int64\n",
      "df_l_t_a : AgeGroup\n",
      "0     708\n",
      "1    3474\n",
      "2    4375\n",
      "dtype: int64\n",
      "df_l_t_g : AgeGroup\n",
      "0     726\n",
      "1    3390\n",
      "2    4488\n",
      "dtype: int64\n",
      "df_l_b_g : AgeGroup\n",
      "0    481\n",
      "1    666\n",
      "2    858\n",
      "dtype: int64\n",
      "df_l_b_c : AgeGroup\n",
      "0    514\n",
      "1    777\n",
      "2    937\n",
      "dtype: int64\n",
      "df_f_t_a : AgeGroup\n",
      "0     668\n",
      "1    2316\n",
      "2    2703\n",
      "dtype: int64\n",
      "df_f_b_g : AgeGroup\n",
      "0     831\n",
      "1    1003\n",
      "2    1070\n",
      "dtype: int64\n",
      "df_f_b_c : AgeGroup\n",
      "0    458\n",
      "2    569\n",
      "dtype: int64\n",
      "df_vf_t_a : AgeGroup\n",
      "0    207\n",
      "1    627\n",
      "2    512\n",
      "dtype: int64\n",
      "df_vf_t_g : AgeGroup\n",
      "0    209\n",
      "1    584\n",
      "2    625\n",
      "dtype: int64\n",
      "df_vf_b_g : AgeGroup\n",
      "0    113\n",
      "1    104\n",
      "2     48\n",
      "dtype: int64\n",
      "df_vf_b_c : AgeGroup\n",
      "0    119\n",
      "1    109\n",
      "2     20\n",
      "dtype: int64\n",
      "Species\n",
      "AA    8557\n",
      "AG    8604\n",
      "dtype: int64\n",
      "Species\n",
      "AC    2228\n",
      "AG    2005\n",
      "dtype: int64\n",
      "Species\n",
      "AA    5687\n",
      "dtype: int64\n",
      "Species\n",
      "AC    1027\n",
      "AG    2904\n",
      "dtype: int64\n",
      "Species\n",
      "AA    1346\n",
      "AG    1418\n",
      "dtype: int64\n",
      "Species\n",
      "AC    248\n",
      "AG    265\n",
      "dtype: int64\n",
      "Species\n",
      "AA    2306\n",
      "AC    2318\n",
      "AG    3600\n",
      "dtype: int64\n",
      "Status\n",
      "BF    2162\n",
      "GR    1661\n",
      "SF    4401\n",
      "dtype: int64\n",
      "RearCnd\n",
      "TL    8224\n",
      "dtype: int64\n",
      "AgeGroup\n",
      "0    2624\n",
      "1    2800\n",
      "2    2800\n",
      "dtype: int64\n",
      "Species\n",
      "AA    14945\n",
      "AC     3677\n",
      "AG    13873\n",
      "dtype: int64\n",
      "Status\n",
      "BF     6441\n",
      "GR     5076\n",
      "SF    17701\n",
      "UN     3277\n",
      "dtype: int64\n",
      "RearCnd\n",
      "TF     9618\n",
      "TL    19600\n",
      "VF     3277\n",
      "dtype: int64\n",
      "AgeGroup\n",
      "0     3663\n",
      "1    12561\n",
      "2    16271\n",
      "dtype: int64\n",
      "shape of X : (8224, 1625)\n",
      "shape of y age : (8224,)\n",
      "shape of y age groups : (8224,)\n",
      "shape of y species : (8224,)\n",
      "shape of y status : (8224,)\n",
      "shape of X_vf : (32495, 1625)\n",
      "shape of y_age_vf age : (32495,)\n",
      "shape of y_age_groups_vf : (32495,)\n",
      "shape of y y_species_vf : (32495,)\n",
      "shape of y y_status_vf : (32495,)\n"
     ]
    }
   ],
   "source": [
    "SelectFreqs = False\n",
    "\n",
    "df = pd.read_csv(\"/home/josh/Documents/Mosquito_Project/New_Data/Data/MIMIdata_update_19_02/mosquitoes_country_LM_5_0.dat\", '\\t')\n",
    "df.head(10)\n",
    "RearCnd_counts = df.groupby('RearCnd').size()\n",
    "print('{}'.format(RearCnd_counts))\n",
    "\n",
    "df['AgeGroup'] = 0\n",
    "df['AgeGroup'] = np.where(df['Age']>10, 2, np.where(df['Age']>4, 1, 0))\n",
    "\n",
    "df_vf = df[df['RearCnd']=='VF']\n",
    "df_vf = df_vf[df_vf['Status']=='UN']\n",
    "df = df[df['RearCnd']!='VF']\n",
    "df = df[df['Status']!='UN']\n",
    "df_l = df[df['RearCnd']=='TL']\n",
    "df_l_g = df_l[df_l['Country']=='S']\n",
    "df_l_g_a = df_l_g[df_l_g['Species']=='AA']\n",
    "age_counts = df_l_g_a.groupby('AgeGroup').size()\n",
    "print('df_l_g_a : {}'.format(age_counts))\n",
    "df_l_g_g = df_l_g[df_l_g['Species']=='AG']\n",
    "age_counts = df_l_g_g.groupby('AgeGroup').size()\n",
    "print('df_l_g_g : {}'.format(age_counts))\n",
    "df_l_g_c = df_l_g[df_l_g['Species']=='AC']\n",
    "age_counts = df_l_g_c.groupby('AgeGroup').size()\n",
    "print('df_l_g_c : {}'.format(age_counts))\n",
    "df_l_t = df_l[df_l['Country']=='T']\n",
    "df_l_t_a = df_l_t[df_l_t['Species']=='AA']\n",
    "age_counts = df_l_t_a.groupby('AgeGroup').size()\n",
    "print('df_l_t_a : {}'.format(age_counts))\n",
    "df_l_t_g = df_l_t[df_l_t['Species']=='AG']\n",
    "age_counts = df_l_t_g.groupby('AgeGroup').size()\n",
    "print('df_l_t_g : {}'.format(age_counts))\n",
    "df_l_b = df_l[df_l['Country']=='B']\n",
    "df_l_b_g = df_l_b[df_l_b['Species']=='AG']\n",
    "age_counts = df_l_b_g.groupby('AgeGroup').size()\n",
    "print('df_l_b_g : {}'.format(age_counts))\n",
    "df_l_b_c = df_l_b[df_l_b['Species']=='AC']\n",
    "age_counts = df_l_b_c.groupby('AgeGroup').size()\n",
    "print('df_l_b_c : {}'.format(age_counts))\n",
    "df_f = df[df['RearCnd']=='TF']\n",
    "df_f_t = df_f[df_f['Country']=='T']\n",
    "df_f_t_a = df_f_t[df_f_t['Species']=='AA']\n",
    "age_counts = df_f_t_a.groupby('AgeGroup').size()\n",
    "print('df_f_t_a : {}'.format(age_counts))\n",
    "# df_f_t_g = df_f_t[df_f_t['Species']=='AG'] #There isn't any\n",
    "df_f_b = df_f[df_f['Country']=='B']\n",
    "df_f_b_g = df_f_b[df_f_b['Species']=='AG']\n",
    "age_counts = df_f_b_g.groupby('AgeGroup').size()\n",
    "print('df_f_b_g : {}'.format(age_counts))\n",
    "df_f_b_c = df_f_b[df_f_b['Species']=='AC']\n",
    "age_counts = df_f_b_c.groupby('AgeGroup').size()\n",
    "print('df_f_b_c : {}'.format(age_counts))\n",
    "df_vf_t = df_vf[df_vf['Country']=='T']\n",
    "df_vf_t_a = df_vf_t[df_vf_t['Species']=='AA']\n",
    "age_counts = df_vf_t_a.groupby('AgeGroup').size()\n",
    "print('df_vf_t_a : {}'.format(age_counts))\n",
    "df_vf_t_g = df_vf_t[df_vf_t['Species']=='AG']\n",
    "age_counts = df_vf_t_g.groupby('AgeGroup').size()\n",
    "print('df_vf_t_g : {}'.format(age_counts))\n",
    "df_vf_b = df_vf[df_vf['Country']=='B']\n",
    "df_vf_b_g = df_vf_b[df_vf_b['Species']=='AG']\n",
    "age_counts = df_vf_b_g.groupby('AgeGroup').size()\n",
    "print('df_vf_b_g : {}'.format(age_counts))\n",
    "df_vf_b_c = df_vf_b[df_vf_b['Species']=='AC']\n",
    "age_counts = df_vf_b_c.groupby('AgeGroup').size()\n",
    "print('df_vf_b_c : {}'.format(age_counts))\n",
    "Species_counts = df_l_t.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_l_b.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_f_t.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_f_b.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_vf_t.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_vf_b.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "\n",
    "size_inc = 400\n",
    "for age in range(3):\n",
    "    df_temp = df_l_b_g[df_l_b_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    if age == 0:\n",
    "        df_train = df_temp.iloc[index_df_temp_inc]\n",
    "        df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "    else:\n",
    "        df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "        df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_b_c[df_l_b_c['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 400\n",
    "for age in range(3):\n",
    "    df_temp = df_l_t_a[df_l_t_a['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_t_g[df_l_t_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 400\n",
    "for age in range(3):\n",
    "    df_temp = df_l_g_a[df_l_g_a['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_g_g[df_l_g_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_g_c[df_l_g_c['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 0 # 50\n",
    "for age in range(3):\n",
    "    df_temp = df_f_t_a[df_f_t_a['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_f_b_g[df_f_b_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_f_b_c[df_f_b_c['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 0\n",
    "for age in range(3):\n",
    "    df_temp = df_vf_t_a[df_vf_t_a['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_vf_t_g[df_vf_t_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 0\n",
    "for age in range(3):\n",
    "    df_temp = df_vf_b_g[df_vf_b_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_vf_b_c[df_vf_b_c['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "    \n",
    "    \n",
    "Species_counts = df_train.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Status_counts = df_train.groupby('Status').size()\n",
    "print('{}'.format(Status_counts))\n",
    "RearCnd_counts = df_train.groupby('RearCnd').size()\n",
    "print('{}'.format(RearCnd_counts))\n",
    "age_counts = df_train.groupby('AgeGroup').size()\n",
    "print('{}'.format(age_counts))\n",
    "Species_counts = df_test.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Status_counts = df_test.groupby('Status').size()\n",
    "print('{}'.format(Status_counts))\n",
    "RearCnd_counts = df_test.groupby('RearCnd').size()\n",
    "print('{}'.format(RearCnd_counts))\n",
    "age_counts = df_test.groupby('AgeGroup').size()\n",
    "print('{}'.format(age_counts))\n",
    "\n",
    "if SelectFreqs:\n",
    "    X = df_train[[\"3400\",\"3276\",\"2922\",\"2854\",\"1900\",\"1746\",\"1636\",\"1540\",\"1458\",\"1306\",\"1154\",\"1076\",\"1028\",\"880\"]]\n",
    "else:\n",
    "    X = df_train.iloc[:,6:-1]\n",
    "y_age = df_train[\"Age\"]\n",
    "y_age_groups = df_train[\"AgeGroup\"]\n",
    "y_species = df_train[\"Species\"]\n",
    "y_status = df_train[\"Status\"]\n",
    "\n",
    "print('shape of X : {}'.format(X.shape))\n",
    "print('shape of y age : {}'.format(y_age.shape))\n",
    "print('shape of y age groups : {}'.format(y_age_groups.shape))\n",
    "print('shape of y species : {}'.format(y_species.shape))\n",
    "print('shape of y status : {}'.format(y_status.shape))\n",
    "\n",
    "X = np.asarray(X)\n",
    "y_age = np.asarray(y_age)\n",
    "y_age_groups = np.asarray(y_age_groups)\n",
    "y_species = np.asarray(y_species)\n",
    "y_status = np.asarray(y_status)\n",
    "\n",
    "if SelectFreqs:\n",
    "    X_vf = df_test[[\"3400\",\"3276\",\"2922\",\"2854\",\"1900\",\"1746\",\"1636\",\"1540\",\"1458\",\"1306\",\"1154\",\"1076\",\"1028\",\"880\"]]\n",
    "else:\n",
    "    X_vf = df_test.iloc[:,6:-1]\n",
    "y_age_vf = df_test[\"Age\"]\n",
    "y_age_groups_vf = df_test[\"AgeGroup\"]\n",
    "y_species_vf = df_test[\"Species\"]\n",
    "y_status_vf = df_test[\"Status\"]\n",
    "print('shape of X_vf : {}'.format(X_vf.shape))\n",
    "print('shape of y_age_vf age : {}'.format(y_age_vf.shape))\n",
    "print('shape of y_age_groups_vf : {}'.format(y_age_groups_vf.shape))\n",
    "print('shape of y y_species_vf : {}'.format(y_species_vf.shape))\n",
    "print('shape of y y_status_vf : {}'.format(y_status_vf.shape))\n",
    "X_vf = np.asarray(X_vf)\n",
    "y_age_vf = np.asarray(y_age_vf)\n",
    "y_age_groups_vf = np.asarray(y_age_groups_vf)\n",
    "y_species_vf = np.asarray(y_species_vf)\n",
    "y_status_vf = np.asarray(y_status_vf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_dim = len(X[0])\n",
    "\n",
    "# y_age_groups = np.where((y_age<=4), 0, 0)\n",
    "# y_age_groups = np.where((y_age>=5) & (y_age<=10), 1, y_age_groups)\n",
    "# y_age_groups = np.where((y_age>=11), 2, y_age_groups)\n",
    "\n",
    "## Ages transformed\n",
    "# y_age_list = [[age] for age in y_age]\n",
    "y_age_groups_list = [[age] for age in y_age_groups]\n",
    "y_species_list = [[species] for species in y_species]\n",
    "y_status_list = [[status] for status in y_status]\n",
    "# age = MultiLabelBinarizer().fit_transform(np.array(y_age_list))\n",
    "# age_classes = list(np.unique(y_age_list))\n",
    "age_groups = MultiLabelBinarizer().fit_transform(np.array(y_age_groups_list))\n",
    "age_group_classes = [\"1-4\", \"5-10\", \"11-17\"]\n",
    "species = MultiLabelBinarizer().fit_transform(np.array(y_species_list))\n",
    "species_classes = list(np.unique(y_species_list))\n",
    "status = MultiLabelBinarizer().fit_transform(np.array(y_status_list))\n",
    "status_classes = list(np.unique(y_status_list))\n",
    "\n",
    "outdir = \"Results_Paper/\"\n",
    "build_folder(outdir, False)\n",
    "SelectFreqs = False\n",
    "\n",
    "## Labels default - all classification\n",
    "# labels_default, classes_default, outputs_default = [age, age_groups, species], [age_classes, age_group_classes, species_classes], ['xAge', 'xAgeGroup', 'xSpecies']\n",
    "labels_default, classes_default, outputs_default = [age_groups, species], [age_group_classes, species_classes], ['xAgeGroup', 'xSpecies']\n",
    "\n",
    "\n",
    "## Declare and train the model\n",
    "model_size = [{'type':'c', 'filter':16, 'kernel':8, 'stride':1, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':8, 'stride':2, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':3, 'stride':1, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':6, 'stride':2, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':5, 'stride':1, 'pooling':2},\n",
    "             {'type':'d', 'width':500}]\n",
    "\n",
    "## Name the model\n",
    "model_name = 'Baseline_CNN'\n",
    "label = labels_default\n",
    "    \n",
    "## Split data into 10 folds for training/testing\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=rand_seed)\n",
    "            \n",
    "## Features\n",
    "# features = X\n",
    "    \n",
    "histories = []\n",
    "fold = 1\n",
    "train_model = True\n",
    "\n",
    "## Name a folder for the outputs to go into\n",
    "savedir = (outdir+\"Trian_Lab_Only_All/\")\n",
    "build_folder(savedir, True)\n",
    "    \n",
    "start_time = time()\n",
    "save_predicted = []\n",
    "save_true = []\n",
    "\n",
    "## Scale train, test\n",
    "scl = StandardScaler()\n",
    "features_scl = scl.fit(X=X)\n",
    "features = features_scl.transform(X=X)\n",
    "\n",
    "## Split into training / testing\n",
    "test_splits = train_test_split(features, *(label), test_size=0.1, shuffle=True, random_state=rand_seed)\n",
    "## Pack up data\n",
    "X_train = test_splits.pop(0)\n",
    "X_test = test_splits.pop(0)\n",
    "y_train = test_splits[::2]\n",
    "y_test = test_splits[1::2]\n",
    "\n",
    "if not SelectFreqs:\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "\n",
    "model_to_test = {\n",
    "    \"model_shape\" : [model_size], # defines the hidden layers of the model\n",
    "    \"model_name\"  : [model_name],\n",
    "    \"input_layer_dim\"  : [input_layer_dim], # size of input layer\n",
    "    \"model_ver_num\"  : [0],\n",
    "    \"fold\"  : [fold], # kf.split number on\n",
    "    \"labels\"   : [y_train],\n",
    "    \"features\" : [X_train],\n",
    "    \"classes\"  : [classes_default],\n",
    "    \"outputs\"   : [outputs_default],\n",
    "    \"compile_loss\": [{'age': 'categorical_crossentropy'}],\n",
    "    \"compile_metrics\" :[{'age': 'accuracy'}]\n",
    "}\n",
    "\n",
    "## Call function to train all the models from the dictionary\n",
    "model, history = train_models(model_to_test, savedir, SelectFreqs=SelectFreqs)\n",
    "histories.append(history)\n",
    "\n",
    "predicted_labels = list([] for i in range(len(y_train)))\n",
    "true_labels = list([] for i in range(len(y_train)))\n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "predicted_labels = [x+[y] for x,y in zip(predicted_labels,y_predicted)]\n",
    "true_labels = [x+[y] for x,y in zip(true_labels,y_test)]\n",
    "\n",
    "predicted_labels = [predicted_labels[i][0].tolist() for i in range(len(predicted_labels))]\n",
    "true_labels = [true_labels[i][0].tolist() for i in range(len(true_labels))]\n",
    "\n",
    "for pred, tru in zip(predicted_labels, true_labels):\n",
    "    save_predicted.append(pred)\n",
    "    save_true.append(tru)\n",
    "\n",
    "## Visualize the results\n",
    "visualize(histories, savedir, model_name, str(fold), classes_default, outputs_default, predicted_labels, true_labels)\n",
    "# log_data(test_index, 'test_index', fold, savedir)\n",
    "\n",
    "# Clear the Keras session, otherwise it will keep adding new\n",
    "# models to the same TensorFlow graph each time we create\n",
    "# a model with a different set of hyper-parameters.\n",
    "K.clear_session()\n",
    "\n",
    "# Delete the Keras model with these hyper-parameters from memory.\n",
    "del model\n",
    "\n",
    "# visualize(1, savedir, model_name, \"Averaged\", classes_default, outputs_default, save_predicted, save_true)\n",
    "end_time = time()\n",
    "print('Run time : {} s'.format(end_time-start_time))\n",
    "print('Run time : {} m'.format((end_time-start_time)/60))\n",
    "print('Run time : {} h'.format((end_time-start_time)/3600))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(histories, savedir, model_name, str(fold), classes_default, outputs_default, predicted_labels, true_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "input_layer_dim = len(X[0])\n",
    "\n",
    "## Ages transformed\n",
    "y_age_groups_list = [[age] for age in y_age_groups]\n",
    "y_species_list = [[species] for species in y_species]\n",
    "y_status_list = [[status] for status in y_status]\n",
    "\n",
    "age_groups = MultiLabelBinarizer().fit_transform(np.array(y_age_groups_list))\n",
    "age_group_classes = [\"1-4\", \"5-10\", \"11-17\"]\n",
    "species = MultiLabelBinarizer().fit_transform(np.array(y_species_list))\n",
    "species_classes = list(np.unique(y_species_list))\n",
    "status = MultiLabelBinarizer().fit_transform(np.array(y_status_list))\n",
    "status_classes = list(np.unique(y_status_list))\n",
    "\n",
    "outdir = \"Results_Paper/\"\n",
    "build_folder(outdir, False)\n",
    "SelectFreqs = False\n",
    "\n",
    "## Labels default - all classification\n",
    "# labels_default, classes_default, outputs_default = [age, age_groups, species], [age_classes, age_group_classes, species_classes], ['xAge', 'xAgeGroup', 'xSpecies']\n",
    "labels_default, classes_default, outputs_default = [age_groups, species], [age_group_classes, species_classes], ['xAgeGroup', 'xSpecies']\n",
    "\n",
    "\n",
    "## Name the model\n",
    "model_name = 'Baseline_CNN'\n",
    "label = labels_default\n",
    "    \n",
    "## Split data into 10 folds for training/testing\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=rand_seed)\n",
    "            \n",
    "## Features\n",
    "# features = X\n",
    "    \n",
    "histories = []\n",
    "fold = 1\n",
    "train_model = False\n",
    "\n",
    "## Name a folder for the outputs to go into\n",
    "savedir = (outdir+\"Trian_Lab_Only_All/\")\n",
    "build_folder(savedir, False)\n",
    "    \n",
    "start_time = time()\n",
    "save_predicted = []\n",
    "save_true = []\n",
    "\n",
    "## Scale train, test\n",
    "scl = StandardScaler()\n",
    "features_scl = scl.fit(X=X)\n",
    "features = features_scl.transform(X=X)\n",
    "\n",
    "## Split into training / testing\n",
    "test_splits = train_test_split(features, *(label), test_size=0.1, shuffle=True, random_state=rand_seed)\n",
    "## Pack up data\n",
    "X_train = test_splits.pop(0)\n",
    "X_test = test_splits.pop(0)\n",
    "y_train = test_splits[::2]\n",
    "y_test = test_splits[1::2]\n",
    "\n",
    "if not SelectFreqs:\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "model = load_model((savedir+\"Baseline_CNN_Model.h5\"))\n",
    "print('Model loaded successfully') \n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "predicted_labels = list([] for i in range(len(y_train)))\n",
    "true_labels = list([] for i in range(len(y_train)))\n",
    "\n",
    "predicted_labels = [x+[y] for x,y in zip(predicted_labels,y_predicted)]\n",
    "true_labels = [x+[y] for x,y in zip(true_labels,y_test)]\n",
    "\n",
    "predicted_labels = [predicted_labels[i][0].tolist() for i in range(len(predicted_labels))]\n",
    "true_labels = [true_labels[i][0].tolist() for i in range(len(true_labels))]\n",
    "\n",
    "for pred, tru in zip(predicted_labels, true_labels):\n",
    "    save_predicted.append(pred)\n",
    "    save_true.append(tru)\n",
    "\n",
    "## Visualize the results\n",
    "visualize(histories, savedir, model_name, str(fold), classes_default, outputs_default, predicted_labels, true_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
