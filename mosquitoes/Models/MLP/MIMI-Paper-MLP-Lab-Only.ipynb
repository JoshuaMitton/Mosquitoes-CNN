{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook provides the functionality to build, train, and test a CNN for predicting mosquito age, grouped age, species, and status.\n",
    "\n",
    "## Structure:\n",
    "* Import packages to be used.\n",
    "* Load mosquito data.\n",
    "* Define fucntions for plotting, visualisation, and logging.\n",
    "* Define a function to build the CNN.\n",
    "* Define a function to train the CNN.\n",
    "* Main section to organise data, define the CNN, and call the building and training of the CNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/anaconda3/envs/mosquitoes/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/josh/anaconda3/envs/mosquitoes/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/josh/anaconda3/envs/mosquitoes/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/josh/anaconda3/envs/mosquitoes/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/josh/anaconda3/envs/mosquitoes/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/josh/anaconda3/envs/mosquitoes/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pylab as pl\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "import pickle\n",
    "import random as rn\n",
    "import os\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib\n",
    "\n",
    "font = {'weight' : 'normal',\n",
    "        'size'   : 18}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras import layers, metrics\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.models import model_from_json, load_model\n",
    "from keras.regularizers import *\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras import backend as K\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# rand_seed = np.random.randint(low=0, high=100)\n",
    "rand_seed = 16\n",
    "print(rand_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "## The below is necessary for starting Numpy generated random numbers in a well-defined initial state.\n",
    "np.random.seed(rand_seed)\n",
    "\n",
    "## The below is necessary for starting core Python generated random numbers in a well-defined state.\n",
    "rn.seed(12345)\n",
    "\n",
    "## Force TensorFlow to use single thread.\n",
    "## Multiple threads are a potential source of\n",
    "## non-reproducible results.\n",
    "## For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "# session_conf = tf.ConfigProto(device_count = {'GPU':0}, intra_op_parallelism_threads=4) #session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "# session_conf = tf.ConfigProto(device_count = {'GPU':0}) #session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "#session_conf.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "## The below tf.set_random_seed() will make random number generation\n",
    "## in the TensorFlow backend have a well-defined initial state.\n",
    "## For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.35)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "The data file is created using Loco Mosquito:\n",
    "https://github.com/magonji/MIMI-project/blob/master/Loco%20mosquito%204.0.ipynb\n",
    "\n",
    "### The data file has headings: Species - Status - RearCnd - Age - Country- Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moved to each model run section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function used to create a new folder for the CNN outputs.\n",
    "Useful to stop forgetting to name a new folder when trying out a new model varient and overwriting a days training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_folder(fold, to_build = False):\n",
    "    if not os.path.isdir(fold):\n",
    "        if to_build == True:\n",
    "            os.mkdir(fold)\n",
    "        else:\n",
    "            print('Directory does not exists, not creating directory!')\n",
    "    else:\n",
    "        if to_build == True:\n",
    "            raise NameError('Directory already exists, cannot be created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for plotting confusion matrcies\n",
    "This normalizes the confusion matrix and ensures neat plotting for all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, output, save_path, model_name, fold,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          printout=False):\n",
    "\n",
    "    font = {'weight' : 'normal',\n",
    "            'size'   : 15}\n",
    "\n",
    "    matplotlib.rc('font', **font)\n",
    "\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        if printout:\n",
    "            print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        if printout:\n",
    "            print('Confusion matrix, without normalization')\n",
    "\n",
    "    if printout:\n",
    "        print(cm)\n",
    "    \n",
    "    plt.figure(figsize=(2.8,2.8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, vmin=0, vmax=1) # np.max(np.sum(cm, axis=1)))\n",
    "#     plt.title([title+' - '+model_name])\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.ylim(2.5,-0.5)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout(pad=0.3)\n",
    "#     plt.tight_layout()\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')\n",
    "    plt.savefig((save_path+\"Confusion_Matrix_\"+model_name+\"_\"+fold+\"_\"+output[1:]+\".pdf\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function used for visualizing outputs\n",
    "This splits the output data into the four categories before plotting the confusion matricies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for visualizing losses and metrics once the neural network fold is trained\n",
    "def visualize(histories, save_path, model_name, fold, classes, outputs, predicted, true):\n",
    "    # Sort out predictions and true labels\n",
    "    for label_predictions_arr, label_true_arr, classes, outputs in zip(predicted, true, classes, outputs):\n",
    "        classes_pred = np.argmax(label_predictions_arr, axis=-1)\n",
    "        classes_true = np.argmax(label_true_arr, axis=-1)\n",
    "        cnf_matrix = confusion_matrix(classes_true, classes_pred)\n",
    "        plot_confusion_matrix(cnf_matrix, classes, outputs, save_path, model_name, fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for logging data associated with the model\n",
    "def log_data(log, name, fold, save_path):\n",
    "    f = open((save_path+name+'_'+str(fold)+'_log.txt'), 'w')\n",
    "    np.savetxt(f, log)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fucntion for graphing the training data\n",
    "This fucntion creates tidy graphs of loss and accuracy as the models are training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_history(history, model_name, model_ver_num, fold, save_path):\n",
    "\n",
    "    font = {'weight' : 'normal',\n",
    "            'size'   : 18}\n",
    "\n",
    "    matplotlib.rc('font', **font)\n",
    "    \n",
    "    #not_validation = list(filter(lambda x: x[0:3] != \"val\", history.history.keys()))\n",
    "#     print('history.history.keys : {}'.format(history.history.keys()))\n",
    "    filtered = filter(lambda x: x[0:3] != \"val\", history.history.keys())\n",
    "    not_validation = list(filtered)\n",
    "    for i in not_validation:\n",
    "        plt.figure(figsize=(15,7))\n",
    "#         plt.title(i+\"/ \"+\"val_\"+i)\n",
    "        plt.plot(history.history[i], label=i)\n",
    "        plt.plot(history.history[\"val_\"+i], label=\"val_\"+i)\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(i)\n",
    "        plt.savefig(save_path +model_name+\"_\"+str(model_ver_num)+\"_\"+str(fold)+\"_\"+i)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funciton to create the CNN\n",
    "This function takes as an input a list of dictionaries. Each element in the list is a new hidden layer in the model. For each layer the dictionary defines the layer to be used.\n",
    "\n",
    "### Available options are:\n",
    "Convolutional Layer:\n",
    "* type = 'c'\n",
    "* filter = optional number of filters\n",
    "* kernel = optional size of the filters\n",
    "* stride = optional size of stride to take between filters\n",
    "* pooling = optional width of the max pooling\n",
    "* {'type':'c', 'filter':16, 'kernel':5, 'stride':1, 'pooling':2}\n",
    "\n",
    "dense layer:\n",
    "* type = 'd'\n",
    "* width = option width of the layer\n",
    "* {'type':'d', 'width':500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchDataset(Dataset):\n",
    "    \"\"\"Torch mosquito dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, x, y_age, y_species):\n",
    "        self.x = x\n",
    "        self.y_age = y_age\n",
    "        self.y_species = y_species\n",
    "        \n",
    "#         print(f'x shape : {x.shape}')\n",
    "#         print(f'y_age shape : {y_age.shape}')\n",
    "#         print(f'y_species shape : {y_species.shape}')\n",
    "#         print(f'x len : {len(x)}')\n",
    "#         print(f'y_age len : {len(y_age)}')\n",
    "#         print(f'y_species len : {len(y_species)}')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_age)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = [self.x[idx], self.y_age[idx], self.y_species[idx]]\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_feats):\n",
    "        super(TorchModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_feats, 500)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(500, 500)\n",
    "        self.fc31 = nn.Linear(500, 3)\n",
    "        self.fc32 = nn.Linear(500, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        y_a = self.fc31(x)\n",
    "        y_s = self.fc32(x)\n",
    "        return y_a, y_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(model_shape, input_layer_dim, SelectFreqs):\n",
    "    \n",
    "    regConst = 0.02\n",
    "    sgd = keras.optimizers.SGD(lr=0.003, decay=1e-5, momentum=0.9, nesterov=True, clipnorm=1.)\n",
    "    cce = 'categorical_crossentropy'\n",
    "\n",
    "    if SelectFreqs:\n",
    "        input_vec = Input(name='input', shape=(input_layer_dim,))\n",
    "        xd = Dense(name='d1', units=500, activation='relu', \n",
    "                     kernel_regularizer=l2(regConst), \n",
    "                     kernel_initializer='he_normal')(input_vec)\n",
    "        xd = BatchNormalization(name='batchnorm_1')(xd)\n",
    "        \n",
    "    else:\n",
    "        input_vec = Input(name='input', shape=(input_layer_dim,1))\n",
    "        \n",
    "        xd = Flatten()(input_vec)\n",
    "\n",
    "        xd = Dropout(name=('dout1'), rate=0.5)(xd)\n",
    "        xd = Dense(name=('d1'), units=500, activation='relu', \n",
    "         kernel_regularizer=l2(regConst), \n",
    "         kernel_initializer='he_normal')(xd)\n",
    "        xd = BatchNormalization(name=('batchnorm_1'))(xd)\n",
    "        \n",
    "        xd = Dropout(name=('dout2'), rate=0.5)(xd)\n",
    "        xd = Dense(name=('d2'), units=500, activation='relu', \n",
    "         kernel_regularizer=l2(regConst), \n",
    "         kernel_initializer='he_normal')(xd)\n",
    "        xd = BatchNormalization(name=('batchnorm_2'))(xd)\n",
    "        \n",
    "    \n",
    "    xAgeGroup     = Dense(name = 'age_group', units = 3, \n",
    "                     activation = 'softmax', \n",
    "                     kernel_regularizer = l2(regConst), \n",
    "                     kernel_initializer = 'he_normal')(xd)\n",
    "    xSpecies = Dense(name ='species', units = 3, \n",
    "                     activation = 'softmax', \n",
    "                     kernel_regularizer = l2(regConst), \n",
    "                     kernel_initializer = 'he_normal')(xd)\n",
    "\n",
    "    outputs = []\n",
    "#     for i in ['xAge', 'xAgeGroup', 'xSpecies']:\n",
    "    for i in ['xAgeGroup', 'xSpecies']:\n",
    "        outputs.append(locals()[i])\n",
    "    model = Model(inputs = input_vec, outputs = outputs)\n",
    "    \n",
    "    model.compile(loss=cce, metrics=['acc'], \n",
    "                  optimizer=sgd)\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(model_to_test, save_path, SelectFreqs=False):\n",
    "    \n",
    "    out_path = save_path+'out/'\n",
    "    build_folder(out_path, True)\n",
    "\n",
    "    model_shape = model_to_test[\"model_shape\"][0]\n",
    "    model_name = model_to_test[\"model_name\"][0]\n",
    "#     input_layer_dim = model_to_test[\"input_layer_dim\"][0]\n",
    "    model_ver_num = model_to_test[\"model_ver_num\"][0]\n",
    "    fold = model_to_test[\"fold\"][0]\n",
    "    label = model_to_test[\"labels\"][0]\n",
    "    features = model_to_test[\"features\"][0]\n",
    "    classes = model_to_test[\"classes\"][0]\n",
    "    outputs = model_to_test[\"outputs\"][0]\n",
    "    compile_loss = model_to_test[\"compile_loss\"][0]\n",
    "    compile_metrics = model_to_test[\"compile_metrics\"][0]\n",
    "\n",
    "#     ## Split into training / testing\n",
    "#     test_splits = train_test_split(features, *(label), test_size=0.1, shuffle=True, random_state=rand_seed)\n",
    "#     ## Pack up data\n",
    "#     X_train = test_splits.pop(0)\n",
    "#     X_val = test_splits.pop(0)\n",
    "#     y_train = test_splits[::2]\n",
    "#     y_val = test_splits[1::2]\n",
    "    \n",
    "#     out_model = create_models(model_shape, input_layer_dim, SelectFreqs)\n",
    "#     out_model.summary()\n",
    "#     out_history = out_model.fit(x = X_train, \n",
    "#                             y = y_train,\n",
    "#                             batch_size = 128*16, \n",
    "#                             verbose = 0, \n",
    "#                             epochs = 8000,\n",
    "#                             validation_data = (X_val, y_val),\n",
    "#                             callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "#                                         patience=400, verbose=0, mode='auto'), \n",
    "#                                         CSVLogger(save_path+model_name+\"_\"+str(model_ver_num)+'.csv', append=True, separator=';')])\n",
    "#     scores = out_model.evaluate(X_val, y_val)\n",
    "#     print(out_model.metrics_names)\n",
    "\n",
    "    ## Kfold training\n",
    "    seed = rand_seed\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "    ## Split data into test and train\n",
    "    \n",
    "    model_ver_num = 0\n",
    "    cv_scores = []\n",
    "    best_score = 0\n",
    "    for train_index, val_index in kfold.split(features):\n",
    "        print('Fold {} Running'.format(model_ver_num))\n",
    "        \n",
    "        X_train, X_val = features[train_index], features[val_index]\n",
    "        y_train, y_val = list(map(lambda y:y[train_index], label)), list(map(lambda y:y[val_index], label))\n",
    "\n",
    "        model = create_models(model_shape, input_layer_dim, SelectFreqs)\n",
    "        if model_ver_num == 0:\n",
    "            model.summary()\n",
    "\n",
    "        history = model.fit(x = X_train, \n",
    "                            y = y_train,\n",
    "                            batch_size = 128*16, \n",
    "                            verbose = 0, \n",
    "                            epochs = 8000,\n",
    "                            validation_data = (X_val, y_val),\n",
    "                            callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                        patience=400, verbose=0, mode='auto'), \n",
    "                                        CSVLogger(out_path+model_name+\"_\"+str(model_ver_num)+'.csv', append=True, separator=';')])\n",
    "        scores = model.evaluate(X_val, y_val)\n",
    "        model.save((out_path+model_name+\"_\"+str(model_ver_num)+'_Model.h5'))\n",
    "        graph_history(history, model_name, model_ver_num, 0, out_path)\n",
    "#         print(model.metrics_names)\n",
    "#         print(scores)\n",
    "        if (scores[3] + scores[4]) > best_score:\n",
    "            out_model = model\n",
    "            out_history = history\n",
    "        \n",
    "        model_ver_num += 1\n",
    "        \n",
    "#         # Clear the Keras session, otherwise it will keep adding new\n",
    "#         # models to the same TensorFlow graph each time we create\n",
    "#         # a model with a different set of hyper-parameters.\n",
    "#         K.clear_session()\n",
    "\n",
    "#         # Delete the Keras model with these hyper-parameters from memory.\n",
    "#         del model\n",
    "        \n",
    "    out_model.save((save_path+'Best_Model.h5'))\n",
    "    graph_history(out_history, 'Best_Model', 0, 0, save_path)\n",
    "    \n",
    "    return out_model, out_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to train the model\n",
    "\n",
    "This function will split the data into training and validation and call the create models function. This fucntion returns the model and training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_models(model_to_test, save_path, SelectFreqs=False):\n",
    "    \n",
    "#     out_path = save_path+'out/'\n",
    "#     build_folder(out_path, True)\n",
    "\n",
    "#     model_shape = model_to_test[\"model_shape\"][0]\n",
    "#     model_name = model_to_test[\"model_name\"][0]\n",
    "# #     input_layer_dim = model_to_test[\"input_layer_dim\"][0]\n",
    "#     model_ver_num = model_to_test[\"model_ver_num\"][0]\n",
    "#     fold = model_to_test[\"fold\"][0]\n",
    "#     label = model_to_test[\"labels\"][0]\n",
    "#     features = model_to_test[\"features\"][0]\n",
    "#     classes = model_to_test[\"classes\"][0]\n",
    "#     outputs = model_to_test[\"outputs\"][0]\n",
    "#     compile_loss = model_to_test[\"compile_loss\"][0]\n",
    "#     compile_metrics = model_to_test[\"compile_metrics\"][0]\n",
    "\n",
    "#     ## Kfold training\n",
    "#     seed = rand_seed\n",
    "#     kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "#     ## Split data into test and train\n",
    "    \n",
    "#     model_ver_num = 0\n",
    "#     cv_scores = []\n",
    "#     best_score = 0\n",
    "#     for train_index, val_index in kfold.split(features):\n",
    "#         print('Fold {} Running'.format(model_ver_num))\n",
    "        \n",
    "#         X_train, X_val = features[train_index], features[val_index]\n",
    "#         y_train, y_val = np.array(list(map(lambda y:y[train_index], label))), np.array(list(map(lambda y:y[val_index], label)))\n",
    "        \n",
    "#         y_age_train = y_train[0]\n",
    "#         y_species_train = y_train[1]\n",
    "#         y_age_val = y_val[0]\n",
    "#         y_species_val = y_val[1]\n",
    "        \n",
    "#         X_train = X_train.astype(np.float32)\n",
    "#         X_val = X_val.astype(np.float32)\n",
    "#         y_age_train = y_age_train.astype(np.float32)\n",
    "#         y_species_train = y_species_train.astype(np.float32)\n",
    "#         y_age_val = y_age_val.astype(np.float32)\n",
    "#         y_species_val = y_species_val.astype(np.float32)\n",
    "        \n",
    "#         trainset = TorchDataset(X_train,y_age_train,y_species_train)\n",
    "#         trainloader = torch.utils.data.DataLoader(trainset, batch_size=128*16, shuffle=True)\n",
    "#         valset = TorchDataset(X_val,y_age_val,y_species_val)\n",
    "#         valloader = torch.utils.data.DataLoader(valset, batch_size=128*16, shuffle=True)\n",
    "        \n",
    "#         model = TorchModel(1625)\n",
    "#         model = model.cuda()\n",
    "        \n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer = torch.optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "        \n",
    "        \n",
    "#         model.train()\n",
    "#         for epochs in tqdm(range(8000)):\n",
    "#             running_loss = 0.0\n",
    "#             running_acc_a = 0\n",
    "#             running_acc_s = 0\n",
    "#             for data in trainloader:\n",
    "#                 # get the inputs; data is a list of [inputs, labels]\n",
    "#                 inputs, labels_a, labels_s = data\n",
    "#                 inputs = torch.squeeze(inputs, dim=-1)\n",
    "#                 inputs = inputs.cuda()\n",
    "#                 labels_a = labels_a.cuda()\n",
    "#                 labels_s = labels_s.cuda()\n",
    "\n",
    "#                 # zero the parameter gradients\n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#                 # forward + backward + optimize\n",
    "\n",
    "#                 out_a, out_s = model(inputs)\n",
    "#                 loss = criterion(out_a, labels_a)\n",
    "#                 loss = loss + criterion(out_s, labels_s)\n",
    "                \n",
    "#                 # Accuracy age\n",
    "#                 pred_a = torch.argmax(out_a, dim=-1)\n",
    "#                 targ_a = torch.argmax(labels_a, dim=-1)\n",
    "#                 acc_a = torch.sum(pred_a == targ_a)\n",
    "                \n",
    "#                 # Accuracy species\n",
    "#                 pred_s = torch.argmax(out_s, dim=-1)\n",
    "#                 targ_s = torch.argmax(labels_s, dim=-1)\n",
    "#                 acc_s = torch.sum(pred_s == targ_s)\n",
    "                \n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "                \n",
    "#                 running_loss += loss.item() * len(inputs)\n",
    "#                 running_acc_a += acc_a.item()\n",
    "#                 running_acc_s += acc_s.item()\n",
    "            \n",
    "#             loss_epoch = running_loss / len(trainset)\n",
    "#             accuracy_a = running_acc_a / len(trainset)\n",
    "#             accuracy_s = running_acc_s / len(trainset)\n",
    "# #             print(f'epoch {epochs} - loss {loss_epoch} - acc age {accuracy_a} - acc species {accuracy_s}')\n",
    "\n",
    "\n",
    "#         model.eval()\n",
    "#         running_loss = 0.0\n",
    "#         running_acc_a = 0\n",
    "#         running_acc_s = 0\n",
    "#         for data in valloader:\n",
    "#             # get the inputs; data is a list of [inputs, labels]\n",
    "#             inputs, labels_a, labels_s = data\n",
    "#             inputs = torch.squeeze(inputs, dim=-1)\n",
    "#             inputs = inputs.cuda()\n",
    "#             labels_a = labels_a.cuda()\n",
    "#             labels_s = labels_s.cuda()\n",
    "\n",
    "#             # forward + backward + optimize\n",
    "#             out_a, out_s = model(inputs)\n",
    "#             loss = criterion(out_a, labels_a)\n",
    "#             loss = loss + criterion(out_s, labels_s)\n",
    "            \n",
    "#             # Accuracy age\n",
    "#             pred_a = torch.argmax(out_a, dim=-1)\n",
    "#             targ_a = torch.argmax(labels_a, dim=-1)\n",
    "#             acc_a = torch.sum(pred_a == targ_a)\n",
    "\n",
    "#             # Accuracy species\n",
    "#             pred_s = torch.argmax(out_s, dim=-1)\n",
    "#             targ_s = torch.argmax(labels_s, dim=-1)\n",
    "#             acc_s = torch.sum(pred_s == targ_s)\n",
    "        \n",
    "#             running_loss += loss.item() * len(inputs)\n",
    "            \n",
    "#         loss_val = running_loss / len(valset)\n",
    "#         accuracy_a = running_acc_a / len(valset)\n",
    "#         accuracy_s = running_acc_s / len(valset)\n",
    "        \n",
    "#         print(f'Validation - loss {loss_val} - acc age {accuracy_a} - acc species {accuracy_s}')\n",
    "            \n",
    "#         torch.save(model.state_dict(), out_path+model_name+\"_\"+str(model_ver_num)+'_Model.h5')\n",
    "        \n",
    "#         if (accuracy_a + accuracy_s) > best_score:\n",
    "#             out_model = model\n",
    "#             out_history = history\n",
    "    \n",
    "#     torch.save(out_model.state_dict(), save_path+'Best_Model.h5')\n",
    "# #     graph_history(out_history, 'Best_Model', 0, 0, save_path)\n",
    "        \n",
    "\n",
    "# #         model = create_models(model_shape, input_layer_dim, SelectFreqs)\n",
    "# #         if model_ver_num == 0:\n",
    "# #             model.summary()\n",
    "\n",
    "# #         history = model.fit(x = X_train, \n",
    "# #                             y = y_train,\n",
    "# #                             batch_size = 128*16, \n",
    "# #                             verbose = 0, \n",
    "# #                             epochs = 8000,\n",
    "# #                             validation_data = (X_val, y_val),\n",
    "# #                             callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "# #                                         patience=400, verbose=0, mode='auto'), \n",
    "# #                                         CSVLogger(out_path+model_name+\"_\"+str(model_ver_num)+'.csv', append=True, separator=';')])\n",
    "# #         scores = model.evaluate(X_val, y_val)\n",
    "# #         model.save((out_path+model_name+\"_\"+str(model_ver_num)+'_Model.h5'))\n",
    "# #         graph_history(history, model_name, model_ver_num, 0, out_path)\n",
    "# # #         print(model.metrics_names)\n",
    "# # #         print(scores)\n",
    "# #         if (scores[3] + scores[4]) > best_score:\n",
    "# #             out_model = model\n",
    "# #             out_history = history\n",
    "        \n",
    "# #         model_ver_num += 1\n",
    "        \n",
    "# # #         # Clear the Keras session, otherwise it will keep adding new\n",
    "# # #         # models to the same TensorFlow graph each time we create\n",
    "# # #         # a model with a different set of hyper-parameters.\n",
    "# # #         K.clear_session()\n",
    "\n",
    "# # #         # Delete the Keras model with these hyper-parameters from memory.\n",
    "# # #         del model\n",
    "        \n",
    "# #     out_model.save((save_path+'Best_Model.h5'))\n",
    "# #     graph_history(out_history, 'Best_Model', 0, 0, save_path)\n",
    "    \n",
    "#     return out_model, out_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main section\n",
    "\n",
    "Functionality:\n",
    "* Oganises the data into a format of lists of data, classes, labels.\n",
    "* Define the CNN to be built.\n",
    "* Define the KFold validation to be used.\n",
    "* Build a folder to output data into.\n",
    "* Standardize and oragnise data into training/testing.\n",
    "* Call the model training.\n",
    "* Organize outputs and call visualization for plotting and graphing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "Directory already exists, cannot be created!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b19892e064ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Results_Paper/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbuild_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-448de7f87972>\u001b[0m in \u001b[0;36mbuild_folder\u001b[0;34m(fold, to_build)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mto_build\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Directory already exists, cannot be created!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: Directory already exists, cannot be created!"
     ]
    }
   ],
   "source": [
    "outdir = \"Results_Paper/\"\n",
    "build_folder(outdir, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training All Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RearCnd\n",
      "TF     9618\n",
      "TL    27838\n",
      "VF     3285\n",
      "dtype: int64\n",
      "df_l_g_a : AgeGroup\n",
      "0    306\n",
      "1    592\n",
      "2    763\n",
      "dtype: int64\n",
      "df_l_g_g : AgeGroup\n",
      "0    629\n",
      "1    827\n",
      "2    821\n",
      "dtype: int64\n",
      "df_l_g_c : AgeGroup\n",
      "0     318\n",
      "1     892\n",
      "2    1282\n",
      "dtype: int64\n",
      "df_l_t_a : AgeGroup\n",
      "0     708\n",
      "1    3474\n",
      "2    4375\n",
      "dtype: int64\n",
      "df_l_t_g : AgeGroup\n",
      "0     726\n",
      "1    3390\n",
      "2    4488\n",
      "dtype: int64\n",
      "df_l_b_g : AgeGroup\n",
      "0    481\n",
      "1    666\n",
      "2    858\n",
      "dtype: int64\n",
      "df_l_b_c : AgeGroup\n",
      "0    514\n",
      "1    777\n",
      "2    937\n",
      "dtype: int64\n",
      "df_f_t_a : AgeGroup\n",
      "0     668\n",
      "1    2316\n",
      "2    2703\n",
      "dtype: int64\n",
      "df_f_b_g : AgeGroup\n",
      "0     831\n",
      "1    1003\n",
      "2    1070\n",
      "dtype: int64\n",
      "df_f_b_c : AgeGroup\n",
      "0    458\n",
      "2    569\n",
      "dtype: int64\n",
      "df_vf_t_a : AgeGroup\n",
      "0    207\n",
      "1    627\n",
      "2    512\n",
      "dtype: int64\n",
      "df_vf_t_g : AgeGroup\n",
      "0    209\n",
      "1    584\n",
      "2    625\n",
      "dtype: int64\n",
      "df_vf_b_g : AgeGroup\n",
      "0    113\n",
      "1    104\n",
      "2     48\n",
      "dtype: int64\n",
      "df_vf_b_c : AgeGroup\n",
      "0    119\n",
      "1    109\n",
      "2     20\n",
      "dtype: int64\n",
      "Species\n",
      "AA    8557\n",
      "AG    8604\n",
      "dtype: int64\n",
      "Species\n",
      "AC    2228\n",
      "AG    2005\n",
      "dtype: int64\n",
      "Species\n",
      "AA    5687\n",
      "dtype: int64\n",
      "Species\n",
      "AC    1027\n",
      "AG    2904\n",
      "dtype: int64\n",
      "Species\n",
      "AA    1346\n",
      "AG    1418\n",
      "dtype: int64\n",
      "Species\n",
      "AC    248\n",
      "AG    265\n",
      "dtype: int64\n",
      "Species\n",
      "AA    2306\n",
      "AC    2318\n",
      "AG    3600\n",
      "dtype: int64\n",
      "Status\n",
      "BF    2162\n",
      "GR    1661\n",
      "SF    4401\n",
      "dtype: int64\n",
      "RearCnd\n",
      "TL    8224\n",
      "dtype: int64\n",
      "AgeGroup\n",
      "0    2624\n",
      "1    2800\n",
      "2    2800\n",
      "dtype: int64\n",
      "Species\n",
      "AA    14945\n",
      "AC     3677\n",
      "AG    13873\n",
      "dtype: int64\n",
      "Status\n",
      "BF     6441\n",
      "GR     5076\n",
      "SF    17701\n",
      "UN     3277\n",
      "dtype: int64\n",
      "RearCnd\n",
      "TF     9618\n",
      "TL    19600\n",
      "VF     3277\n",
      "dtype: int64\n",
      "AgeGroup\n",
      "0     3663\n",
      "1    12561\n",
      "2    16271\n",
      "dtype: int64\n",
      "shape of X : (8224, 1625)\n",
      "shape of y age : (8224,)\n",
      "shape of y age groups : (8224,)\n",
      "shape of y species : (8224,)\n",
      "shape of y status : (8224,)\n",
      "shape of X_vf : (32495, 1625)\n",
      "shape of y_age_vf age : (32495,)\n",
      "shape of y_age_groups_vf : (32495,)\n",
      "shape of y y_species_vf : (32495,)\n",
      "shape of y y_status_vf : (32495,)\n"
     ]
    }
   ],
   "source": [
    "SelectFreqs = False\n",
    "\n",
    "df = pd.read_csv(\"../../Data/mosquitoes_country_LM_5_0.dat\", '\\t')\n",
    "df.head(10)\n",
    "RearCnd_counts = df.groupby('RearCnd').size()\n",
    "print('{}'.format(RearCnd_counts))\n",
    "\n",
    "df['AgeGroup'] = 0\n",
    "df['AgeGroup'] = np.where(df['Age']>10, 2, np.where(df['Age']>4, 1, 0))\n",
    "\n",
    "df_vf = df[df['RearCnd']=='VF']\n",
    "df_vf = df_vf[df_vf['Status']=='UN']\n",
    "df = df[df['RearCnd']!='VF']\n",
    "df = df[df['Status']!='UN']\n",
    "df_l = df[df['RearCnd']=='TL']\n",
    "df_l_g = df_l[df_l['Country']=='S']\n",
    "df_l_g_a = df_l_g[df_l_g['Species']=='AA']\n",
    "age_counts = df_l_g_a.groupby('AgeGroup').size()\n",
    "print('df_l_g_a : {}'.format(age_counts))\n",
    "df_l_g_g = df_l_g[df_l_g['Species']=='AG']\n",
    "age_counts = df_l_g_g.groupby('AgeGroup').size()\n",
    "print('df_l_g_g : {}'.format(age_counts))\n",
    "df_l_g_c = df_l_g[df_l_g['Species']=='AC']\n",
    "age_counts = df_l_g_c.groupby('AgeGroup').size()\n",
    "print('df_l_g_c : {}'.format(age_counts))\n",
    "df_l_t = df_l[df_l['Country']=='T']\n",
    "df_l_t_a = df_l_t[df_l_t['Species']=='AA']\n",
    "age_counts = df_l_t_a.groupby('AgeGroup').size()\n",
    "print('df_l_t_a : {}'.format(age_counts))\n",
    "df_l_t_g = df_l_t[df_l_t['Species']=='AG']\n",
    "age_counts = df_l_t_g.groupby('AgeGroup').size()\n",
    "print('df_l_t_g : {}'.format(age_counts))\n",
    "df_l_b = df_l[df_l['Country']=='B']\n",
    "df_l_b_g = df_l_b[df_l_b['Species']=='AG']\n",
    "age_counts = df_l_b_g.groupby('AgeGroup').size()\n",
    "print('df_l_b_g : {}'.format(age_counts))\n",
    "df_l_b_c = df_l_b[df_l_b['Species']=='AC']\n",
    "age_counts = df_l_b_c.groupby('AgeGroup').size()\n",
    "print('df_l_b_c : {}'.format(age_counts))\n",
    "df_f = df[df['RearCnd']=='TF']\n",
    "df_f_t = df_f[df_f['Country']=='T']\n",
    "df_f_t_a = df_f_t[df_f_t['Species']=='AA']\n",
    "age_counts = df_f_t_a.groupby('AgeGroup').size()\n",
    "print('df_f_t_a : {}'.format(age_counts))\n",
    "# df_f_t_g = df_f_t[df_f_t['Species']=='AG'] #There isn't any\n",
    "df_f_b = df_f[df_f['Country']=='B']\n",
    "df_f_b_g = df_f_b[df_f_b['Species']=='AG']\n",
    "age_counts = df_f_b_g.groupby('AgeGroup').size()\n",
    "print('df_f_b_g : {}'.format(age_counts))\n",
    "df_f_b_c = df_f_b[df_f_b['Species']=='AC']\n",
    "age_counts = df_f_b_c.groupby('AgeGroup').size()\n",
    "print('df_f_b_c : {}'.format(age_counts))\n",
    "df_vf_t = df_vf[df_vf['Country']=='T']\n",
    "df_vf_t_a = df_vf_t[df_vf_t['Species']=='AA']\n",
    "age_counts = df_vf_t_a.groupby('AgeGroup').size()\n",
    "print('df_vf_t_a : {}'.format(age_counts))\n",
    "df_vf_t_g = df_vf_t[df_vf_t['Species']=='AG']\n",
    "age_counts = df_vf_t_g.groupby('AgeGroup').size()\n",
    "print('df_vf_t_g : {}'.format(age_counts))\n",
    "df_vf_b = df_vf[df_vf['Country']=='B']\n",
    "df_vf_b_g = df_vf_b[df_vf_b['Species']=='AG']\n",
    "age_counts = df_vf_b_g.groupby('AgeGroup').size()\n",
    "print('df_vf_b_g : {}'.format(age_counts))\n",
    "df_vf_b_c = df_vf_b[df_vf_b['Species']=='AC']\n",
    "age_counts = df_vf_b_c.groupby('AgeGroup').size()\n",
    "print('df_vf_b_c : {}'.format(age_counts))\n",
    "Species_counts = df_l_t.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_l_b.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_f_t.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_f_b.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_vf_t.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Species_counts = df_vf_b.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "\n",
    "size_inc = 400\n",
    "for age in range(3):\n",
    "    df_temp = df_l_b_g[df_l_b_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    if age == 0:\n",
    "        df_train = df_temp.iloc[index_df_temp_inc]\n",
    "        df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "    else:\n",
    "        df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "        df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_b_c[df_l_b_c['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 400\n",
    "for age in range(3):\n",
    "    df_temp = df_l_t_a[df_l_t_a['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_t_g[df_l_t_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 400\n",
    "for age in range(3):\n",
    "    df_temp = df_l_g_a[df_l_g_a['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_g_g[df_l_g_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_g_c[df_l_g_c['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 0 # 50\n",
    "for age in range(3):\n",
    "    df_temp = df_f_t_a[df_f_t_a['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_f_b_g[df_f_b_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_f_b_c[df_f_b_c['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 0\n",
    "for age in range(3):\n",
    "    df_temp = df_vf_t_a[df_vf_t_a['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_vf_t_g[df_vf_t_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 0\n",
    "for age in range(3):\n",
    "    df_temp = df_vf_b_g[df_vf_b_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_vf_b_c[df_vf_b_c['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(rand_seed)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "    \n",
    "    \n",
    "Species_counts = df_train.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Status_counts = df_train.groupby('Status').size()\n",
    "print('{}'.format(Status_counts))\n",
    "RearCnd_counts = df_train.groupby('RearCnd').size()\n",
    "print('{}'.format(RearCnd_counts))\n",
    "age_counts = df_train.groupby('AgeGroup').size()\n",
    "print('{}'.format(age_counts))\n",
    "Species_counts = df_test.groupby('Species').size()\n",
    "print('{}'.format(Species_counts))\n",
    "Status_counts = df_test.groupby('Status').size()\n",
    "print('{}'.format(Status_counts))\n",
    "RearCnd_counts = df_test.groupby('RearCnd').size()\n",
    "print('{}'.format(RearCnd_counts))\n",
    "age_counts = df_test.groupby('AgeGroup').size()\n",
    "print('{}'.format(age_counts))\n",
    "\n",
    "if SelectFreqs:\n",
    "    X = df_train[[\"3400\",\"3276\",\"2922\",\"2854\",\"1900\",\"1746\",\"1636\",\"1540\",\"1458\",\"1306\",\"1154\",\"1076\",\"1028\",\"880\"]]\n",
    "else:\n",
    "    X = df_train.iloc[:,6:-1]\n",
    "y_age = df_train[\"Age\"]\n",
    "y_age_groups = df_train[\"AgeGroup\"]\n",
    "y_species = df_train[\"Species\"]\n",
    "y_status = df_train[\"Status\"]\n",
    "\n",
    "print('shape of X : {}'.format(X.shape))\n",
    "print('shape of y age : {}'.format(y_age.shape))\n",
    "print('shape of y age groups : {}'.format(y_age_groups.shape))\n",
    "print('shape of y species : {}'.format(y_species.shape))\n",
    "print('shape of y status : {}'.format(y_status.shape))\n",
    "\n",
    "X = np.asarray(X)\n",
    "y_age = np.asarray(y_age)\n",
    "y_age_groups = np.asarray(y_age_groups)\n",
    "y_species = np.asarray(y_species)\n",
    "y_status = np.asarray(y_status)\n",
    "\n",
    "if SelectFreqs:\n",
    "    X_vf = df_test[[\"3400\",\"3276\",\"2922\",\"2854\",\"1900\",\"1746\",\"1636\",\"1540\",\"1458\",\"1306\",\"1154\",\"1076\",\"1028\",\"880\"]]\n",
    "else:\n",
    "    X_vf = df_test.iloc[:,6:-1]\n",
    "y_age_vf = df_test[\"Age\"]\n",
    "y_age_groups_vf = df_test[\"AgeGroup\"]\n",
    "y_species_vf = df_test[\"Species\"]\n",
    "y_status_vf = df_test[\"Status\"]\n",
    "print('shape of X_vf : {}'.format(X_vf.shape))\n",
    "print('shape of y_age_vf age : {}'.format(y_age_vf.shape))\n",
    "print('shape of y_age_groups_vf : {}'.format(y_age_groups_vf.shape))\n",
    "print('shape of y y_species_vf : {}'.format(y_species_vf.shape))\n",
    "print('shape of y y_status_vf : {}'.format(y_status_vf.shape))\n",
    "X_vf = np.asarray(X_vf)\n",
    "y_age_vf = np.asarray(y_age_vf)\n",
    "y_age_groups_vf = np.asarray(y_age_groups_vf)\n",
    "y_species_vf = np.asarray(y_species_vf)\n",
    "y_status_vf = np.asarray(y_status_vf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Running\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 1625, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1625)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dout1 (Dropout)                 (None, 1625)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "d1 (Dense)                      (None, 500)          813000      dout1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_1 (BatchNormalization (None, 500)          2000        d1[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "dout2 (Dropout)                 (None, 500)          0           batchnorm_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "d2 (Dense)                      (None, 500)          250500      dout2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_2 (BatchNormalization (None, 500)          2000        d2[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "age_group (Dense)               (None, 3)            1503        batchnorm_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "species (Dense)                 (None, 3)            1503        batchnorm_2[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,070,506\n",
      "Trainable params: 1,068,506\n",
      "Non-trainable params: 2,000\n",
      "__________________________________________________________________________________________________\n",
      "741/741 [==============================] - 0s 61us/step\n",
      "Fold 1 Running\n",
      "740/740 [==============================] - 0s 56us/step\n",
      "Fold 2 Running\n",
      "740/740 [==============================] - 0s 57us/step\n",
      "Fold 3 Running\n",
      "740/740 [==============================] - 0s 64us/step\n",
      "Fold 4 Running\n",
      "740/740 [==============================] - 0s 61us/step\n",
      "Fold 5 Running\n",
      "740/740 [==============================] - 0s 67us/step\n",
      "Fold 6 Running\n",
      "740/740 [==============================] - 0s 70us/step\n",
      "Fold 7 Running\n",
      "740/740 [==============================] - 0s 61us/step\n",
      "Fold 8 Running\n",
      "740/740 [==============================] - 0s 74us/step\n",
      "Fold 9 Running\n",
      "740/740 [==============================] - 0s 64us/step\n",
      "Run time : 8240.406684875488 s\n",
      "Run time : 137.34011141459146 m\n",
      "Run time : 2.289001856909858 h\n"
     ]
    }
   ],
   "source": [
    "input_layer_dim = len(X[0])\n",
    "\n",
    "# y_age_groups = np.where((y_age<=4), 0, 0)\n",
    "# y_age_groups = np.where((y_age>=5) & (y_age<=10), 1, y_age_groups)\n",
    "# y_age_groups = np.where((y_age>=11), 2, y_age_groups)\n",
    "\n",
    "## Ages transformed\n",
    "# y_age_list = [[age] for age in y_age]\n",
    "y_age_groups_list = [[age] for age in y_age_groups]\n",
    "y_species_list = [[species] for species in y_species]\n",
    "y_status_list = [[status] for status in y_status]\n",
    "# age = MultiLabelBinarizer().fit_transform(np.array(y_age_list))\n",
    "# age_classes = list(np.unique(y_age_list))\n",
    "age_groups = MultiLabelBinarizer().fit_transform(np.array(y_age_groups_list))\n",
    "age_group_classes = [\"1-4\", \"5-10\", \"11-17\"]\n",
    "species = MultiLabelBinarizer().fit_transform(np.array(y_species_list))\n",
    "species_classes = list(np.unique(y_species_list))\n",
    "status = MultiLabelBinarizer().fit_transform(np.array(y_status_list))\n",
    "status_classes = list(np.unique(y_status_list))\n",
    "\n",
    "outdir = \"Results_Paper/\"\n",
    "build_folder(outdir, False)\n",
    "SelectFreqs = False\n",
    "\n",
    "## Labels default - all classification\n",
    "# labels_default, classes_default, outputs_default = [age, age_groups, species], [age_classes, age_group_classes, species_classes], ['xAge', 'xAgeGroup', 'xSpecies']\n",
    "labels_default, classes_default, outputs_default = [age_groups, species], [age_group_classes, species_classes], ['xAgeGroup', 'xSpecies']\n",
    "\n",
    "\n",
    "## Declare and train the model\n",
    "model_size = [{'type':'c', 'filter':16, 'kernel':8, 'stride':1, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':8, 'stride':2, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':3, 'stride':1, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':6, 'stride':2, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':5, 'stride':1, 'pooling':2},\n",
    "             {'type':'d', 'width':500}]\n",
    "\n",
    "## Name the model\n",
    "model_name = 'Baseline_CNN'\n",
    "label = labels_default\n",
    "    \n",
    "## Split data into 10 folds for training/testing\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=rand_seed)\n",
    "            \n",
    "## Features\n",
    "# features = X\n",
    "    \n",
    "histories = []\n",
    "fold = 1\n",
    "train_model = True\n",
    "\n",
    "## Name a folder for the outputs to go into\n",
    "savedir = (outdir+\"Trian_Lab_Only_All/\")\n",
    "build_folder(savedir, True)\n",
    "    \n",
    "start_time = time()\n",
    "save_predicted = []\n",
    "save_true = []\n",
    "\n",
    "## Scale train, test\n",
    "scl = StandardScaler()\n",
    "features_scl = scl.fit(X=X)\n",
    "features = features_scl.transform(X=X)\n",
    "\n",
    "## Split into training / testing\n",
    "test_splits = train_test_split(features, *(label), test_size=0.1, shuffle=True, random_state=rand_seed)\n",
    "## Pack up data\n",
    "X_train = test_splits.pop(0)\n",
    "X_test = test_splits.pop(0)\n",
    "y_train = test_splits[::2]\n",
    "y_test = test_splits[1::2]\n",
    "\n",
    "if not SelectFreqs:\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "\n",
    "model_to_test = {\n",
    "    \"model_shape\" : [model_size], # defines the hidden layers of the model\n",
    "    \"model_name\"  : [model_name],\n",
    "    \"input_layer_dim\"  : [input_layer_dim], # size of input layer\n",
    "    \"model_ver_num\"  : [0],\n",
    "    \"fold\"  : [fold], # kf.split number on\n",
    "    \"labels\"   : [y_train],\n",
    "    \"features\" : [X_train],\n",
    "    \"classes\"  : [classes_default],\n",
    "    \"outputs\"   : [outputs_default],\n",
    "    \"compile_loss\": [{'age': 'categorical_crossentropy'}],\n",
    "    \"compile_metrics\" :[{'age': 'accuracy'}]\n",
    "}\n",
    "\n",
    "## Call function to train all the models from the dictionary\n",
    "model, history = train_models(model_to_test, savedir, SelectFreqs=SelectFreqs)\n",
    "histories.append(history)\n",
    "\n",
    "predicted_labels = list([] for i in range(len(y_train)))\n",
    "true_labels = list([] for i in range(len(y_train)))\n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "predicted_labels = [x+[y] for x,y in zip(predicted_labels,y_predicted)]\n",
    "true_labels = [x+[y] for x,y in zip(true_labels,y_test)]\n",
    "\n",
    "predicted_labels = [predicted_labels[i][0].tolist() for i in range(len(predicted_labels))]\n",
    "true_labels = [true_labels[i][0].tolist() for i in range(len(true_labels))]\n",
    "\n",
    "for pred, tru in zip(predicted_labels, true_labels):\n",
    "    save_predicted.append(pred)\n",
    "    save_true.append(tru)\n",
    "\n",
    "## Visualize the results\n",
    "visualize(histories, savedir, model_name, str(fold), classes_default, outputs_default, predicted_labels, true_labels)\n",
    "# log_data(test_index, 'test_index', fold, savedir)\n",
    "\n",
    "# Clear the Keras session, otherwise it will keep adding new\n",
    "# models to the same TensorFlow graph each time we create\n",
    "# a model with a different set of hyper-parameters.\n",
    "K.clear_session()\n",
    "\n",
    "# Delete the Keras model with these hyper-parameters from memory.\n",
    "del model\n",
    "\n",
    "# visualize(1, savedir, model_name, \"Averaged\", classes_default, outputs_default, save_predicted, save_true)\n",
    "end_time = time()\n",
    "print('Run time : {} s'.format(end_time-start_time))\n",
    "print('Run time : {} m'.format((end_time-start_time)/60))\n",
    "print('Run time : {} h'.format((end_time-start_time)/3600))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(histories, savedir, model_name, str(fold), classes_default, outputs_default, predicted_labels, true_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "input_layer_dim = len(X[0])\n",
    "\n",
    "## Ages transformed\n",
    "y_age_groups_list = [[age] for age in y_age_groups]\n",
    "y_species_list = [[species] for species in y_species]\n",
    "y_status_list = [[status] for status in y_status]\n",
    "\n",
    "age_groups = MultiLabelBinarizer().fit_transform(np.array(y_age_groups_list))\n",
    "age_group_classes = [\"1-4\", \"5-10\", \"11-17\"]\n",
    "species = MultiLabelBinarizer().fit_transform(np.array(y_species_list))\n",
    "species_classes = list(np.unique(y_species_list))\n",
    "status = MultiLabelBinarizer().fit_transform(np.array(y_status_list))\n",
    "status_classes = list(np.unique(y_status_list))\n",
    "\n",
    "outdir = \"Results_Paper/\"\n",
    "build_folder(outdir, False)\n",
    "SelectFreqs = False\n",
    "\n",
    "## Labels default - all classification\n",
    "# labels_default, classes_default, outputs_default = [age, age_groups, species], [age_classes, age_group_classes, species_classes], ['xAge', 'xAgeGroup', 'xSpecies']\n",
    "labels_default, classes_default, outputs_default = [age_groups, species], [age_group_classes, species_classes], ['xAgeGroup', 'xSpecies']\n",
    "\n",
    "\n",
    "## Name the model\n",
    "model_name = 'Baseline_CNN'\n",
    "label = labels_default\n",
    "    \n",
    "## Split data into 10 folds for training/testing\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=rand_seed)\n",
    "            \n",
    "## Features\n",
    "# features = X\n",
    "    \n",
    "histories = []\n",
    "fold = 1\n",
    "train_model = False\n",
    "\n",
    "## Name a folder for the outputs to go into\n",
    "savedir = (outdir+\"Trian_Lab_Only_All/\")\n",
    "build_folder(savedir, False)\n",
    "    \n",
    "start_time = time()\n",
    "save_predicted = []\n",
    "save_true = []\n",
    "\n",
    "## Scale train, test\n",
    "scl = StandardScaler()\n",
    "features_scl = scl.fit(X=X)\n",
    "features = features_scl.transform(X=X)\n",
    "\n",
    "## Split into training / testing\n",
    "test_splits = train_test_split(features, *(label), test_size=0.1, shuffle=True, random_state=rand_seed)\n",
    "## Pack up data\n",
    "X_train = test_splits.pop(0)\n",
    "X_test = test_splits.pop(0)\n",
    "y_train = test_splits[::2]\n",
    "y_test = test_splits[1::2]\n",
    "\n",
    "if not SelectFreqs:\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "model = load_model((savedir+\"Baseline_CNN_Model.h5\"))\n",
    "print('Model loaded successfully') \n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "predicted_labels = list([] for i in range(len(y_train)))\n",
    "true_labels = list([] for i in range(len(y_train)))\n",
    "\n",
    "predicted_labels = [x+[y] for x,y in zip(predicted_labels,y_predicted)]\n",
    "true_labels = [x+[y] for x,y in zip(true_labels,y_test)]\n",
    "\n",
    "predicted_labels = [predicted_labels[i][0].tolist() for i in range(len(predicted_labels))]\n",
    "true_labels = [true_labels[i][0].tolist() for i in range(len(true_labels))]\n",
    "\n",
    "for pred, tru in zip(predicted_labels, true_labels):\n",
    "    save_predicted.append(pred)\n",
    "    save_true.append(tru)\n",
    "\n",
    "## Visualize the results\n",
    "visualize(histories, savedir, model_name, str(fold), classes_default, outputs_default, predicted_labels, true_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
